{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e74d452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_dataset(domain_path, output_path, total_samples, seed=42):\n",
    "    random.seed(seed)\n",
    "    start_time = time.time()\n",
    "    print(\"⏳ Starting dataset generation...\")\n",
    "\n",
    "    # Load domain requirements\n",
    "    with open(domain_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        DOMAIN_REQUIREMENTS = json.load(f)\n",
    "    domains = list(DOMAIN_REQUIREMENTS.keys())\n",
    "\n",
    "    # Track label counts\n",
    "    label_counts = defaultdict(int)\n",
    "    domain_label_counts = {d: defaultdict(int) for d in domains}\n",
    "\n",
    "    # --- Generate a candidate profile ---\n",
    "    def generate_candidate_profile(domain):\n",
    "        req = DOMAIN_REQUIREMENTS[domain]\n",
    "\n",
    "        # Work experience\n",
    "        work_exp = []\n",
    "        if random.random() > 0.3:  # 30% chance to be fresher\n",
    "            for _ in range(random.randint(1, 3)):\n",
    "                work_exp.append({\n",
    "                    \"title\": random.choice(req.get(\"job_titles\", [\"Professional\"])),\n",
    "                    \"years\": random.randint(1, 3)\n",
    "                })\n",
    "\n",
    "        # Skills\n",
    "        skills = random.sample(\n",
    "            req[\"skills\"],\n",
    "            k=random.randint(max(2, len(req[\"skills\"]) // 2), len(req[\"skills\"]))\n",
    "        )\n",
    "        other_skills = [\"communication\", \"teamwork\", \"problem solving\", \"critical thinking\"]\n",
    "        skills += random.sample(other_skills, k=random.randint(0, 2))\n",
    "\n",
    "        # Test score\n",
    "        test_score = random.randint(req[\"min_score\"], 95)\n",
    "\n",
    "        return {\n",
    "            \"skills\": skills,\n",
    "            \"work_experience\": work_exp,\n",
    "            \"test_score\": test_score,\n",
    "            \"preferred_domain\": domain\n",
    "        }\n",
    "\n",
    "    # --- Evaluate candidate profile ---\n",
    "    def evaluate_candidate(candidate):\n",
    "        domain = candidate[\"preferred_domain\"]\n",
    "        req = DOMAIN_REQUIREMENTS[domain]\n",
    "\n",
    "        skills = set(candidate[\"skills\"])\n",
    "        work_exp = candidate[\"work_experience\"]\n",
    "        total_years = sum(job[\"years\"] for job in work_exp)\n",
    "        relevant_exp = sum(job[\"years\"] for job in work_exp if job[\"title\"] in req.get(\"job_titles\", []))\n",
    "        test_score = candidate[\"test_score\"]\n",
    "\n",
    "        matched_skills = skills.intersection(req[\"skills\"])\n",
    "        skill_ratio = len(matched_skills) / max(1, len(req[\"skills\"]))\n",
    "\n",
    "        # --- Threshold-based label assignment ---\n",
    "        if test_score >= 80 and skill_ratio > 0.35 and relevant_exp > 1:\n",
    "            return \"fit\"\n",
    "        elif test_score >= 70 and skill_ratio > 0.25 and relevant_exp >= 1:\n",
    "            return \"partial\"\n",
    "        else:\n",
    "            # Check if candidate fits another domain\n",
    "            for alt_domain, alt_req in DOMAIN_REQUIREMENTS.items():\n",
    "                if alt_domain == domain:\n",
    "                    continue\n",
    "                if skills.intersection(alt_req[\"skills\"]) and test_score >= alt_req[\"min_score\"]:\n",
    "                    return \"suggest\"\n",
    "            return \"no_fit\"\n",
    "\n",
    "    # --- Generate dataset ---\n",
    "    dataset = []\n",
    "    for i in range(total_samples):\n",
    "        domain = random.choice(domains)\n",
    "        candidate = generate_candidate_profile(domain)\n",
    "        candidate[\"label\"] = evaluate_candidate(candidate)\n",
    "        dataset.append(candidate)\n",
    "\n",
    "        label_counts[candidate[\"label\"]] += 1\n",
    "        domain_label_counts[domain][candidate[\"label\"]] += 1\n",
    "\n",
    "        # Progress log\n",
    "        if (i + 1) % max(1, total_samples // 10) == 0:\n",
    "            print(f\"🔹 {i + 1}/{total_samples} samples generated\")\n",
    "\n",
    "    # Save dataset\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n📊 Label counts:\", dict(label_counts))\n",
    "    print(\"📊 Domain-wise label counts:\")\n",
    "    for domain, counts in domain_label_counts.items():\n",
    "        print(f\"{domain}: {dict(counts)}\")\n",
    "\n",
    "    print(f\"\\n✅ Dataset generation finished in {time.time() - start_time:.2f} seconds.\")\n",
    "    return dataset, label_counts, domain_label_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a7b2f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Starting dataset generation...\n",
      "🔹 50/500 samples generated\n",
      "🔹 100/500 samples generated\n",
      "🔹 150/500 samples generated\n",
      "🔹 200/500 samples generated\n",
      "🔹 250/500 samples generated\n",
      "🔹 300/500 samples generated\n",
      "🔹 350/500 samples generated\n",
      "🔹 400/500 samples generated\n",
      "🔹 450/500 samples generated\n",
      "🔹 500/500 samples generated\n",
      "\n",
      "📊 Label counts: {'suggest': 121, 'no_fit': 79, 'fit': 159, 'partial': 141}\n",
      "📊 Domain-wise label counts:\n",
      "Data Science: {'suggest': 6, 'fit': 7, 'partial': 6, 'no_fit': 1}\n",
      "Web Development: {'partial': 4, 'no_fit': 14, 'fit': 10}\n",
      "UI/UX Design: {'partial': 10, 'fit': 7, 'no_fit': 7, 'suggest': 4}\n",
      "Cybersecurity: {'suggest': 10, 'fit': 9, 'partial': 3}\n",
      "Cloud Computing: {'fit': 13, 'partial': 11, 'suggest': 6, 'no_fit': 4}\n",
      "Artificial Intelligence: {'partial': 7, 'fit': 23, 'suggest': 3, 'no_fit': 1}\n",
      "Mobile App Development: {'no_fit': 10, 'partial': 5, 'fit': 7}\n",
      "DevOps: {'fit': 10, 'partial': 7, 'suggest': 6}\n",
      "Database Administration: {'partial': 10, 'no_fit': 13, 'fit': 7}\n",
      "Networking: {'suggest': 5, 'no_fit': 3, 'fit': 4, 'partial': 8}\n",
      "Digital Marketing: {'suggest': 12, 'no_fit': 10, 'fit': 5, 'partial': 7}\n",
      "Business Analysis: {'suggest': 15, 'partial': 10, 'fit': 4, 'no_fit': 1}\n",
      "Game Development: {'suggest': 9, 'partial': 4, 'fit': 9, 'no_fit': 3}\n",
      "Robotics: {'fit': 8, 'partial': 11, 'no_fit': 2, 'suggest': 6}\n",
      "Blockchain: {'fit': 7, 'partial': 6, 'suggest': 8}\n",
      "Augmented Reality / Virtual Reality: {'suggest': 7, 'partial': 11, 'fit': 12}\n",
      "Quality Assurance: {'fit': 6, 'partial': 5, 'no_fit': 6, 'suggest': 3}\n",
      "Technical Writing: {'suggest': 8, 'no_fit': 4, 'fit': 3, 'partial': 5}\n",
      "Product Management: {'fit': 8, 'suggest': 13, 'partial': 11}\n",
      "\n",
      "✅ Dataset generation finished in 0.03 seconds.\n"
     ]
    }
   ],
   "source": [
    "dataset, label_counts, domain_label_counts = generate_dataset(\n",
    "    domain_path=\"domain_requirements.json\",\n",
    "    output_path=\"dataset.json\",\n",
    "    total_samples=500,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65dd56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time  # For timing measurements\n",
    "\n",
    "\n",
    "# --- Helper to build normalization map ---\n",
    "def build_normalization_map(DOMAIN_REQUIREMENTS):\n",
    "    \"\"\"\n",
    "    Build a dictionary of skill/title aliases for normalization.\n",
    "    Example: {\"node.js\": \"node.js\", \"node\": \"node.js\", \"NodeJs\": \"node.js\"}\n",
    "    \"\"\"\n",
    "    alias_map = {}\n",
    "\n",
    "    def add_alias(word, canonical):\n",
    "        word_norm = word.strip().lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\".\", \"\")\n",
    "        alias_map[word_norm] = canonical.lower()\n",
    "\n",
    "    for domain, req in DOMAIN_REQUIREMENTS.items():\n",
    "        for skill in req.get(\"skills\", []):\n",
    "            canonical = skill\n",
    "            # Add direct and common variations\n",
    "            add_alias(skill, canonical)\n",
    "            add_alias(skill.replace(\".\", \"\"), canonical)\n",
    "            add_alias(skill.replace(\"-\", \"\"), canonical)\n",
    "            add_alias(skill.replace(\" \", \"\"), canonical)\n",
    "            add_alias(skill.lower(), canonical)\n",
    "\n",
    "        for title in req.get(\"job_titles\", []):\n",
    "            canonical = title\n",
    "            add_alias(title, canonical)\n",
    "            add_alias(title.lower(), canonical)\n",
    "\n",
    "    return alias_map\n",
    "\n",
    "\n",
    "# --- Normalize a candidate sample ---\n",
    "def normalize_sample(sample, alias_map):\n",
    "    \"\"\"\n",
    "    Normalize skills and job titles in the sample according to alias_map.\n",
    "    \"\"\"\n",
    "    # Normalize skills\n",
    "    normalized_skills = []\n",
    "    for s in sample.get(\"skills\", []):\n",
    "        key = s.strip().lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\".\", \"\")\n",
    "        if key in alias_map:\n",
    "            normalized_skills.append(alias_map[key])\n",
    "        else:\n",
    "            normalized_skills.append(s.lower())\n",
    "    sample[\"skills\"] = list(set(normalized_skills))  # unique\n",
    "\n",
    "    # Normalize job titles\n",
    "    for job in sample.get(\"work_experience\", []):\n",
    "        title = job.get(\"title\", \"\")\n",
    "        key = title.strip().lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\".\", \"\")\n",
    "        if key in alias_map:\n",
    "            job[\"title\"] = alias_map[key]\n",
    "        else:\n",
    "            job[\"title\"] = title.lower()\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "# --- Main dataset update function ---\n",
    "def update_dataset(dataset_path, domain_path, output_csv, output_parquet):\n",
    "    start_time = time.time()\n",
    "    print(\"⏳ Starting dataset update...\")\n",
    "\n",
    "    # Load your dataset JSON\n",
    "    t0 = time.time()\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    t1 = time.time()\n",
    "    print(f\"📂 Loaded dataset JSON ({len(data)} samples) in {t1 - t0:.4f} seconds.\")\n",
    "\n",
    "    # Load domain requirements\n",
    "    t2 = time.time()\n",
    "    with open(domain_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        DOMAIN_REQUIREMENTS = json.load(f)\n",
    "    t3 = time.time()\n",
    "    print(f\"📂 Loaded domain requirements in {t3 - t2:.4f} seconds.\")\n",
    "\n",
    "    # Build normalization map\n",
    "    alias_map = build_normalization_map(DOMAIN_REQUIREMENTS)\n",
    "\n",
    "    # Normalize data before feature extraction\n",
    "    normalized_data = []\n",
    "    for i, sample in enumerate(data):\n",
    "        normalized_data.append(normalize_sample(sample, alias_map))\n",
    "        if (i + 1) % max(1, len(data)//10) == 0:\n",
    "            print(f\"🔹 Normalized {i + 1}/{len(data)} samples...\")\n",
    "\n",
    "    # Helper: convert JSON sample to ML-ready features (no projects anymore)\n",
    "    def json_to_row(sample):\n",
    "        skills = set(sample.get(\"skills\", []))\n",
    "        work_exp = sample.get(\"work_experience\", [])\n",
    "        total_years = sum(job.get(\"years\", 0) for job in work_exp)\n",
    "        titles = [job.get(\"title\", \"\") for job in work_exp]\n",
    "\n",
    "        domain = sample.get(\"preferred_domain\", \"\")\n",
    "        domain_req = DOMAIN_REQUIREMENTS.get(domain, {})\n",
    "\n",
    "        matched_skills = skills.intersection([s.lower() for s in domain_req.get(\"skills\", [])])\n",
    "        skill_match_ratio = len(matched_skills) / max(1, len(domain_req.get(\"skills\", [])))\n",
    "\n",
    "        relevant_experience = sum(\n",
    "            job.get(\"years\", 0) for job in work_exp if job.get(\"title\", \"\").lower() in [t.lower() for t in domain_req.get(\"job_titles\", [])]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"test_score\": sample.get(\"test_score\", 0),\n",
    "            \"skill_match_ratio\": skill_match_ratio,\n",
    "            \"relevant_experience\": relevant_experience,\n",
    "            \"preferred_domain\": domain.lower(),\n",
    "            \"skills_text\": \" \".join(skills).lower(),\n",
    "            \"titles_text\": \" \".join(titles).lower(),\n",
    "            \"label\": sample.get(\"label\", \"\").lower()\n",
    "        }\n",
    "\n",
    "    # Build dataframe\n",
    "    t_build_start = time.time()\n",
    "    rows = []\n",
    "    for i, x in enumerate(normalized_data):\n",
    "        rows.append(json_to_row(x))\n",
    "        if (i + 1) % max(1, len(normalized_data)//10) == 0:\n",
    "            print(f\"🔹 Processed {i + 1}/{len(normalized_data)} samples...\")\n",
    "    df = pd.DataFrame(rows)\n",
    "    t_build_end = time.time()\n",
    "    print(f\"⏱ Dataframe built in {t_build_end - t_build_start:.2f} seconds.\")\n",
    "\n",
    "    # Save both CSV and Parquet\n",
    "    t_save_start = time.time()\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    df.to_parquet(output_parquet, index=False)\n",
    "    t_save_end = time.time()\n",
    "    print(f\"💾 Dataset saved as CSV and Parquet in {t_save_end - t_save_start:.4f} seconds.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"✅ Total process finished in {end_time - start_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c878d4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Starting dataset update...\n",
      "📂 Loaded dataset JSON (500 samples) in 0.0269 seconds.\n",
      "📂 Loaded domain requirements in 0.0000 seconds.\n",
      "🔹 Normalized 50/500 samples...\n",
      "🔹 Normalized 100/500 samples...\n",
      "🔹 Normalized 150/500 samples...\n",
      "🔹 Normalized 200/500 samples...\n",
      "🔹 Normalized 250/500 samples...\n",
      "🔹 Normalized 300/500 samples...\n",
      "🔹 Normalized 350/500 samples...\n",
      "🔹 Normalized 400/500 samples...\n",
      "🔹 Normalized 450/500 samples...\n",
      "🔹 Normalized 500/500 samples...\n",
      "🔹 Processed 50/500 samples...\n",
      "🔹 Processed 100/500 samples...\n",
      "🔹 Processed 150/500 samples...\n",
      "🔹 Processed 200/500 samples...\n",
      "🔹 Processed 250/500 samples...\n",
      "🔹 Processed 300/500 samples...\n",
      "🔹 Processed 350/500 samples...\n",
      "🔹 Processed 400/500 samples...\n",
      "🔹 Processed 450/500 samples...\n",
      "🔹 Processed 500/500 samples...\n",
      "⏱ Dataframe built in 0.01 seconds.\n",
      "💾 Dataset saved as CSV and Parquet in 0.0000 seconds.\n",
      "✅ Total process finished in 0.04 seconds.\n"
     ]
    }
   ],
   "source": [
    "update_dataset(\n",
    "    dataset_path=\"dataset.json\",       # input dataset generated earlier\n",
    "    domain_path=\"domain_requirements.json\",         # domain requirements\n",
    "    output_csv=\"dataset_clean.csv\",    # output CSV\n",
    "    output_parquet=\"dataset_clean.parquet\"  # output Parquet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e663ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_dataset_no_projects.py\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_dataset(dataset_path, train_output, val_output):\n",
    "    start_time = time.time()\n",
    "    print(\"⏳ Starting dataset preparation...\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. Load dataset\n",
    "    # -------------------------------\n",
    "    t0 = time.time()\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    t1 = time.time()\n",
    "    print(f\"📂 Loaded dataset ({len(df)} samples) in {t1 - t0:.4f} seconds.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Basic cleaning\n",
    "    # -------------------------------\n",
    "    t_clean_start = time.time()\n",
    "    df = df.drop_duplicates()\n",
    "    valid_labels = [\"fit\", \"partial\", \"suggest\", \"no_fit\"]\n",
    "    df = df[df[\"label\"].isin(valid_labels)]\n",
    "\n",
    "    # Fill missing values (remove project related columns)\n",
    "    df = df.fillna({\n",
    "        \"test_score\": 0,\n",
    "        \"skills_text\": \"\",\n",
    "        \"titles_text\": \"\",\n",
    "        \"preferred_domain\": \"\",\n",
    "        \"skill_match_ratio\": 0,\n",
    "        \"relevant_experience\": 0\n",
    "    })\n",
    "\n",
    "    # Clip numeric columns (remove project_match)\n",
    "    df[\"test_score\"] = df[\"test_score\"].clip(0, 100)\n",
    "    df[\"skill_match_ratio\"] = df[\"skill_match_ratio\"].clip(0, 1)\n",
    "    df[\"relevant_experience\"] = df[\"relevant_experience\"].clip(lower=0)\n",
    "\n",
    "    t_clean_end = time.time()\n",
    "    print(f\"🧹 Basic cleaning completed in {t_clean_end - t_clean_start:.4f} seconds.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 3. Text cleaning\n",
    "    # -------------------------------\n",
    "    t_text_start = time.time()\n",
    "    def clean_text(s):\n",
    "        s = re.sub(r\"[^a-z0-9\\s]\", \"\", str(s).lower())\n",
    "        return s.strip()\n",
    "    \n",
    "    for col in [\"skills_text\", \"titles_text\", \"preferred_domain\"]:\n",
    "        df[col] = df[col].apply(clean_text)\n",
    "    t_text_end = time.time()\n",
    "    print(f\"📝 Text cleaning completed in {t_text_end - t_text_start:.4f} seconds.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 4. Manual oversampling to balance labels\n",
    "    # -------------------------------\n",
    "    t_balance_start = time.time()\n",
    "    print(\"Before balancing:\", df[\"label\"].value_counts().to_dict())\n",
    "\n",
    "    max_count = df[\"label\"].value_counts().max()\n",
    "    balanced_dfs = []\n",
    "    for label, group in df.groupby(\"label\"):\n",
    "        n_repeat = max_count // len(group)\n",
    "        remainder = max_count % len(group)\n",
    "        oversampled_group = pd.concat([group]*n_repeat + [group.sample(remainder, random_state=42)])\n",
    "        balanced_dfs.append(oversampled_group)\n",
    "\n",
    "    balanced_df = pd.concat(balanced_dfs).reset_index(drop=True)\n",
    "    print(\"After balancing:\", balanced_df[\"label\"].value_counts().to_dict())\n",
    "    t_balance_end = time.time()\n",
    "    print(f\"⚖️ Oversampling completed in {t_balance_end - t_balance_start:.4f} seconds.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 5. Train/Validation split\n",
    "    # -------------------------------\n",
    "    t_split_start = time.time()\n",
    "    train_df, val_df = train_test_split(\n",
    "        balanced_df,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=balanced_df[\"label\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "    t_split_end = time.time()\n",
    "    print(f\"🔹 Train/Validation split completed in {t_split_end - t_split_start:.4f} seconds.\")\n",
    "\n",
    "    # Save datasets\n",
    "    t_save_start = time.time()\n",
    "    train_df.to_csv(train_output, index=False)\n",
    "    val_df.to_csv(val_output, index=False)\n",
    "    t_save_end = time.time()\n",
    "    print(f\"💾 Train/Validation datasets saved in {t_save_end - t_save_start:.4f} seconds.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"✅ Dataset preparation finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(\"Train size:\", len(train_df), \"Validation size:\", len(val_df))\n",
    "\n",
    "    return train_df, val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13acd911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Starting dataset preparation...\n",
      "📂 Loaded dataset (500 samples) in 0.0111 seconds.\n",
      "🧹 Basic cleaning completed in 0.0065 seconds.\n",
      "📝 Text cleaning completed in 0.0035 seconds.\n",
      "Before balancing: {'fit': 159, 'partial': 141, 'suggest': 121, 'no_fit': 79}\n",
      "After balancing: {'no_fit': 159, 'suggest': 159, 'fit': 159, 'partial': 159}\n",
      "⚖️ Oversampling completed in 0.0062 seconds.\n",
      "🔹 Train/Validation split completed in 0.0000 seconds.\n",
      "💾 Train/Validation datasets saved in 0.0186 seconds.\n",
      "✅ Dataset preparation finished in 0.05 seconds.\n",
      "Train size: 508 Validation size: 128\n"
     ]
    }
   ],
   "source": [
    "# If the function is already in the notebook:\n",
    "train_df, val_df = prepare_dataset(\n",
    "    dataset_path=\"dataset_clean.csv\",     # Input dataset\n",
    "    train_output=\"train_dataset.csv\",    # Where train split will be saved\n",
    "    val_output=\"val_dataset.csv\"         # Where validation split will be saved\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ecf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7184367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b4115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc05053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
