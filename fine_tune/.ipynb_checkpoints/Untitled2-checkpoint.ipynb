{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78911f02-f9b0-484a-88cf-cbc9797e23c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Losses: {'ner': 6.622135419140746}\n",
      "Iteration 2, Losses: {'ner': 2.925693204918483}\n",
      "Iteration 3, Losses: {'ner': 3.9386713766969517}\n",
      "Iteration 4, Losses: {'ner': 3.222151044429803}\n",
      "Iteration 5, Losses: {'ner': 3.1699709109647514}\n",
      "Iteration 6, Losses: {'ner': 3.6515146719725036}\n",
      "Iteration 7, Losses: {'ner': 2.4866343166239124}\n",
      "Iteration 8, Losses: {'ner': 4.101499932592237}\n",
      "Iteration 9, Losses: {'ner': 2.518325702643724}\n",
      "Iteration 10, Losses: {'ner': 2.591015669329465}\n",
      "Iteration 11, Losses: {'ner': 2.835669827058166}\n",
      "Iteration 12, Losses: {'ner': 2.904091538493544}\n",
      "Iteration 13, Losses: {'ner': 2.234317871689236}\n",
      "Iteration 14, Losses: {'ner': 3.3193737687773686}\n",
      "Iteration 15, Losses: {'ner': 2.363161404740587}\n",
      "Iteration 16, Losses: {'ner': 2.20570524669047}\n",
      "Iteration 17, Losses: {'ner': 1.8927399225282469}\n",
      "Iteration 18, Losses: {'ner': 2.2691721898493213}\n",
      "Iteration 19, Losses: {'ner': 7.004281670157811}\n",
      "Iteration 20, Losses: {'ner': 6.6687433088786605}\n",
      "Iteration 21, Losses: {'ner': 3.2272870214657345}\n",
      "Iteration 22, Losses: {'ner': 3.0310087050873933}\n",
      "Iteration 23, Losses: {'ner': 1.9341138143129406}\n",
      "Iteration 24, Losses: {'ner': 3.152845137572342}\n",
      "Iteration 25, Losses: {'ner': 3.7036587393520417}\n",
      "Iteration 26, Losses: {'ner': 2.8767277716853252}\n",
      "Iteration 27, Losses: {'ner': 1.9391101878301953}\n",
      "Iteration 28, Losses: {'ner': 2.770752771114494}\n",
      "Iteration 29, Losses: {'ner': 1.5808674316231084}\n",
      "Iteration 30, Losses: {'ner': 0.8971935218335538}\n",
      "üßë Personal Info:\n",
      "  name: PAVAN CHANDRAPPA HOTTIGOUDRA\n",
      "  email: pavandvh27@gmail.com\n",
      "  phone: +91 7483022523\n",
      "  linkedin: https://linkedin.com/in/pavan\n",
      "  github: https://github.com/pavan\n",
      "\n",
      "‚úÖ Known Skills:\n",
      "  - aws lambda\n",
      "  - express.js\n",
      "  - fastapi\n",
      "  - jwt\n",
      "  - mysql\n",
      "  - node.js\n",
      "  - postman\n",
      "  - react.js\n",
      "  - twilio\n",
      "  - vite\n",
      "\n",
      "‚ùì Unknown Skills:\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import re\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# -------------------------------\n",
    "# 1Ô∏è‚É£ Known skills (expanded)\n",
    "# -------------------------------\n",
    "known_skills = [\n",
    "    \"react.js\", \"node.js\", \"express.js\", \"mysql\", \"mongodb\", \"fastapi\",\n",
    "    \"vite\", \"jwt\", \"postman\", \"twilio\", \"aws lambda\", \"aws cognito\",\n",
    "    \"dynamodb\", \"python\", \"spacy\", \"librosa\", \"bootstrap\", \"java\", \"c\",\n",
    "    \"html5\", \"css3\", \"axios\", \"agile/scrum\", \"responsive design\", \"ds\",\n",
    "    \"system design basics\", \"jsx\", \"rest api\", \"git\", \"github\"\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 2Ô∏è‚É£ Training data for NER\n",
    "# -------------------------------\n",
    "TRAIN_DATA = [\n",
    "    (\"Built APIs with Node.js, Express.js, and MySQL.\", {\"entities\": [(14, 21, \"SKILL\"), (23, 32, \"SKILL\"), (38, 43, \"SKILL\")]}),\n",
    "    (\"Developed cloud functions using AWS Lambda and DynamoDB.\", {\"entities\": [(32, 42, \"SKILL\"), (47, 55, \"SKILL\")]}),\n",
    "    (\"Created web apps with React.js, Vite, and Bootstrap.\", {\"entities\": [(20, 27, \"SKILL\"), (29, 33, \"SKILL\"), (39, 48, \"SKILL\")]}),\n",
    "    (\"Built AI solutions using Python, spaCy, and Librosa.\", {\"entities\": [(24, 30, \"SKILL\"), (32, 37, \"SKILL\"), (43, 50, \"SKILL\")]}),\n",
    "    (\"Implemented authentication using JWT and Postman.\", {\"entities\": [(32, 35, \"SKILL\"), (40, 47, \"SKILL\")]}),\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ Load SpaCy model & prepare NER\n",
    "# -------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "if \"SKILL\" not in ner.labels:\n",
    "    ner.add_label(\"SKILL\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ Fine-tune NER (minibatch)\n",
    "# -------------------------------\n",
    "if TRAIN_DATA:\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.resume_training()\n",
    "        for i in range(30):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(2.0, 16.0, 1.5))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                examples = []\n",
    "                for text, ann in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    examples.append(Example.from_dict(doc, ann))\n",
    "                nlp.update(examples, sgd=optimizer, drop=0.2, losses=losses)\n",
    "            print(f\"Iteration {i+1}, Losses: {losses}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ PhraseMatcher for known skills\n",
    "# -------------------------------\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(skill) for skill in known_skills]\n",
    "matcher.add(\"SKILL\", patterns)\n",
    "\n",
    "# -------------------------------\n",
    "# 6Ô∏è‚É£ Text cleaning (robust)\n",
    "# -------------------------------\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"[\\(\\)|,]\", \" \", text)  # remove (, | ,)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.lower()\n",
    "\n",
    "# -------------------------------\n",
    "# 7Ô∏è‚É£ Extract skills\n",
    "# -------------------------------\n",
    "def extract_skills(text):\n",
    "    text_clean = clean_text(text)\n",
    "    doc = nlp(text_clean)\n",
    "\n",
    "    # PhraseMatcher for known skills\n",
    "    matched_skills = set([doc[start:end].text.lower() for _, start, end in matcher(doc)])\n",
    "\n",
    "    # NER predictions\n",
    "    ner_skills = set([ent.text.lower() for ent in doc.ents if ent.label_ == \"SKILL\"])\n",
    "\n",
    "    # Combine\n",
    "    all_skills = matched_skills.union(ner_skills)\n",
    "\n",
    "    known = sorted([s for s in all_skills if s in known_skills])\n",
    "    unknown = sorted([s for s in all_skills if s not in known_skills])\n",
    "\n",
    "    return known, unknown\n",
    "\n",
    "# -------------------------------\n",
    "# 8Ô∏è‚É£ Extract personal info\n",
    "# -------------------------------\n",
    "def extract_personal_info(text):\n",
    "    text_cleaned = clean_text(text)\n",
    "    lines = text.splitlines()\n",
    "    name = \"\"\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and re.match(r\"^[A-Za-z\\s\\-\\.]+$\", line):\n",
    "            name = line\n",
    "            break\n",
    "    email_match = re.search(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "    email = email_match.group(0) if email_match else \"\"\n",
    "    phone_match = re.search(r\"(\\+?\\d[\\d\\s-]{7,}\\d)\", text)\n",
    "    phone = phone_match.group(0) if phone_match else \"\"\n",
    "    linkedin_match = re.search(r\"https?://(www\\.)?linkedin\\.com/[^\\s,]+\", text)\n",
    "    linkedin = linkedin_match.group(0) if linkedin_match else \"\"\n",
    "    github_match = re.search(r\"https?://(www\\.)?github\\.com/[^\\s,]+\", text)\n",
    "    github = github_match.group(0) if github_match else \"\"\n",
    "    return {\"name\": name, \"email\": email, \"phone\": phone, \"linkedin\": linkedin, \"github\": github}\n",
    "\n",
    "# -------------------------------\n",
    "# 9Ô∏è‚É£ Test on CV\n",
    "# -------------------------------\n",
    "cv_text = \"\"\"\n",
    "PAVAN CHANDRAPPA HOTTIGOUDRA\n",
    "Software Engineer | Full-Stack(MERN) | Rest APIs | AWS | AI & Data-Driven System\n",
    "+91 7483022523 | pavandvh27@gmail.com | https://linkedin.com/in/pavan | https://github.com/pavan\n",
    "Tech Stack: Node.js, Express.js, AWS Lambda, React.js, MySQL, FastAPI, Vite, JWT, Postman, Twilio\n",
    "\"\"\"\n",
    "\n",
    "personal_info = extract_personal_info(cv_text)\n",
    "known_skills_found, unknown_skills_found = extract_skills(cv_text)\n",
    "\n",
    "print(\"üßë Personal Info:\")\n",
    "for k, v in personal_info.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n‚úÖ Known Skills:\")\n",
    "for k in known_skills_found:\n",
    "    print(\"  -\", k)\n",
    "\n",
    "print(\"\\n‚ùì Unknown Skills:\")\n",
    "for u in unknown_skills_found:\n",
    "    print(\"  -\", u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a133964-3e2a-487b-a415-ecaacd0606d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resume 1 ---\n",
      "üßë Personal Info: {'name': 'RAHUL NARAYAN', 'email': 'rahul.narayan@example.com', 'phone': '+91 9876543210', 'linkedin': 'https://linkedin.com/in/rahul', 'github': 'https://github.com/rahul'}\n",
      "‚úÖ Known Skills: ['aws lambda', 'bootstrap', 'dynamodb', 'express.js', 'node.js', 'python', 'react.js', 'spacy', 'vite']\n",
      "‚ùì Unknown Skills: []\n",
      "\n",
      "--- Resume 2 ---\n",
      "üßë Personal Info: {'name': 'SNEHA KUMARI', 'email': 'sneha.k@example.com', 'phone': '+91 9123456780', 'linkedin': 'https://linkedin.com/in/sneha', 'github': 'https://github.com/sneha'}\n",
      "‚úÖ Known Skills: ['bootstrap', 'fastapi', 'jwt', 'librosa', 'mongodb', 'mysql', 'python', 'react.js', 'twilio']\n",
      "‚ùì Unknown Skills: []\n",
      "\n",
      "--- Resume 3 ---\n",
      "üßë Personal Info: {'name': 'ADITYA SHARMA', 'email': 'aditya.sharma@example.com', 'phone': '+91 9988776655', 'linkedin': 'https://linkedin.com/in/aditya', 'github': 'https://github.com/aditya'}\n",
      "‚úÖ Known Skills: ['aws lambda', 'express.js', 'mysql', 'node.js', 'postman', 'react.js', 'vite']\n",
      "‚ùì Unknown Skills: []\n",
      "\n",
      "--- Resume 4 ---\n",
      "üßë Personal Info: {'name': 'PRIYA PATIL', 'email': 'priya.patil@example.com', 'phone': '+91 9876501234', 'linkedin': 'https://linkedin.com/in/priya', 'github': 'https://github.com/priya'}\n",
      "‚úÖ Known Skills: ['aws lambda', 'bootstrap', 'dynamodb', 'express.js', 'jwt', 'librosa', 'mongodb', 'node.js', 'postman', 'python', 'react.js', 'spacy', 'vite']\n",
      "‚ùì Unknown Skills: []\n",
      "\n",
      "--- Resume 5 ---\n",
      "üßë Personal Info: {'name': 'PAVAN CHANDRAPPA HOTTIGOUDRA', 'email': 'pavandvh27@gmail.com', 'phone': '+91 7483022523', 'linkedin': '', 'github': ''}\n",
      "‚úÖ Known Skills: ['agile/scrum', 'aws cognito', 'aws lambda', 'axios', 'bootstrap', 'c', 'css3', 'dynamodb', 'express.js', 'fastapi', 'git', 'github', 'html5', 'java', 'jsx', 'jwt', 'librosa', 'mongodb', 'mysql', 'node.js', 'postman', 'python', 'react.js', 'responsive design', 'rest api', 'spacy', 'system design basics', 'twilio', 'vite']\n",
      "‚ùì Unknown Skills: []\n"
     ]
    }
   ],
   "source": [
    "test_resumes = [\n",
    "    # ---------------- Resume 1 ----------------\n",
    "    \"\"\"\n",
    "RAHUL NARAYAN\n",
    "Software Developer | Cloud-Native Systems | AI Solutions\n",
    "+91 9876543210 | rahul.narayan@example.com | https://linkedin.com/in/rahul | https://github.com/rahul\n",
    "Worked on cloud functions using AWS Lambda and DynamoDB to automate workflows.\n",
    "Developed web applications with React.js, Node.js, and Express.js for better user experience.\n",
    "Implemented AI features using Python and spaCy for text analysis.\n",
    "Created responsive dashboards using Vite and Bootstrap.\n",
    "\"\"\",\n",
    "    # ---------------- Resume 2 ----------------\n",
    "    \"\"\"\n",
    "SNEHA KUMARI\n",
    "Full-Stack Engineer | Backend & AI Integration\n",
    "+91 9123456780 | sneha.k@example.com | https://linkedin.com/in/sneha | https://github.com/sneha\n",
    "Designed APIs with FastAPI and integrated MySQL and MongoDB databases for data management.\n",
    "Built real-time notifications using Twilio and JWT authentication.\n",
    "Developed front-end components using React.js and Bootstrap for mobile-first design.\n",
    "Utilized Python and Librosa for audio signal processing in AI-powered features.\n",
    "\"\"\",\n",
    "    # ---------------- Resume 3 ----------------\n",
    "    \"\"\"\n",
    "ADITYA SHARMA\n",
    "Cloud & Backend Developer\n",
    "+91 9988776655 | aditya.sharma@example.com | https://linkedin.com/in/aditya | https://github.com/aditya\n",
    "Implemented serverless functions using AWS Lambda and secured services with Cognito.\n",
    "Built REST APIs using Node.js and Express.js with MySQL for storing user data.\n",
    "Created interactive web apps using React.js and Vite with reusable components.\n",
    "Integrated Postman for API testing and debugging workflows.\n",
    "\"\"\",\n",
    "    # ---------------- Resume 4 ----------------\n",
    "    \"\"\"\n",
    "PRIYA PATIL\n",
    "AI & Web Developer\n",
    "+91 9876501234 | priya.patil@example.com | https://linkedin.com/in/priya | https://github.com/priya\n",
    "Built AI-powered tools using Python, spaCy, and Librosa to analyze user data.\n",
    "Developed front-end with React.js, Bootstrap, and Vite for responsive UI.\n",
    "Created backend services with Node.js, Express.js, and MongoDB.\n",
    "Implemented authentication and API testing with JWT and Postman.\n",
    "Deployed serverless architecture using AWS Lambda and DynamoDB.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "PAVAN CHANDRAPPA HOTTIGOUDRA\n",
    "Software Engineer | Full-Stack(MERN) | Rest APIs | AWS | AI & Data-Driven System\n",
    "+91 7483022523 | pavandvh27@gmail.com | LinkedIn | GitHub\n",
    "\n",
    "Software  Engineer  with  experience  in  full-stack\n",
    "development,   cloud-native   APIs,   and   AI-driven\n",
    "applications.   Skilled  in  MERN   stack,   RestAPI,\n",
    "React.js,  MongoDB,  and  AWS  (Lambda,  Cognito,\n",
    "DynamoDB) with expertise in secure\n",
    "authentication,   scalable   deployments,   and   data-\n",
    "driven  system  design.  Actively  practicing  DSA  on\n",
    "LeetCode  and  passionate  about  building  robust,\n",
    "efficient, and impactful software solutions.\n",
    "WORK EXPERIENCE\n",
    "\n",
    "Intern ‚Äì Backend Developer\n",
    "Gandeevan Technologies, Bengaluru, India\n",
    "(Hybrid) Jul 2025 ‚Äì Present\n",
    "‚Ä¢ Built  secure  RESTful  APIs  for  authentication\n",
    "and    data    management,    integrating    AWS\n",
    "Cognito for identity and access control.\n",
    "‚Ä¢ Designed  scalable  serverless  workflows  and\n",
    "automated  deployments  via  CI/CD  pipelines,\n",
    "improving reliability and release speed.\n",
    "‚Ä¢ Enhanced  API  security  through  unit  testing,\n",
    "validation, and best practices while\n",
    "collaborating with cross-functional teams.\n",
    "Tech   Stack: Node.js,   Express.js,   AWS   (Cognito,\n",
    "DynamoDB, API Gateway, Lambda), CI/CD, Serverless\n",
    "Architecture.\n",
    "PROJECT\n",
    "\n",
    "Blood Donation Management System\n",
    "‚Ä¢ Developed a scalable SPA using React.js (Vite)\n",
    "with 15+ reusable components, achieving 95%\n",
    "performance and mobile responsiveness.\n",
    "‚Ä¢ Implemented   JWT   auth   with   OTP   reset,\n",
    "securing    all    private    routes    via    Express\n",
    "middleware.\n",
    "‚Ä¢ Built 10+ RESTful APIs with Node.js/Express,\n",
    "integrated  MySQL  to  manage  donors,  blood\n",
    "banks, and requests.\n",
    "Tech: React.js, Node.js, MySQL, Express, Twilio,\n",
    "REST APIs\n",
    "Smart Interview Preparation App\n",
    "‚Ä¢ Built secure RESTful APIs and authentication\n",
    "systems,  integrating  cloud  services  to  ensure\n",
    "scalability and data protection.\n",
    "‚Ä¢ Developed   a   smart   interview   preparation\n",
    "platform with  mock  interviews,  resume-based\n",
    "question   generation,   and   real-time   speech\n",
    "feedback.\n",
    "‚Ä¢ Designed  intuitive  dashboards  and  features  to\n",
    "track performance, progress, and\n",
    "achievements, focusing on user-centric\n",
    "experience.\n",
    "Tech: React.js, FastAPI, MongoDB, Librosa,\n",
    "Whisper, spaCy.\n",
    "EDUCTION\n",
    "\n",
    "Visveshwaraya Technological University\n",
    "Regional Center Mysuru\n",
    "Computer Science and Engineering\n",
    "CGPA : 8.7 (Aug 2022 ‚Äì Jun 2026)\n",
    "Kumadvathi Science and Commerce PU College\n",
    "Shikaripura, Shivamogga, Karnataka\n",
    "Percentage : 95.5% (Jul 2020 ‚Äì Apr 2022)\n",
    "JGSS English Medium School\n",
    "Haveri, Karnataka\n",
    "Percentage: 83% (Jun 2011 ‚Äì Mar 2020)\n",
    "SKILLS\n",
    "\n",
    "Java, AWS, Python,  C,  HTML5,  CSS3,  MySQL,\n",
    "React.js,   Vite,   JSX,   Bootstrap,   Axios,   Node.js,\n",
    "Express.js, REST API, JWT, Git, GitHub, Postman,\n",
    "Agile/Scrum,  Responsive  Design,  DSA,  System\n",
    "Design Basics\n",
    "CERTIFICATIONS\n",
    "\n",
    "Inceptrix   Hackathon   2025 ‚Äì Jain   University\n",
    "Solved    real-world    problems    using    intelligent\n",
    "innovation and rapid prototyping.\n",
    "HACK-2-INTERN ‚Äì VTU, CPGS Mysuru\n",
    "Built real-time software in a team setting; improved\n",
    "debugging and development skills.\n",
    "NPTEL ‚Äì The  Joy  of  Computing  using  Python\n",
    "Learned   Python,   algorithms,   file   handling,   and\n",
    "problem-solving fundamentals.\n",
    "Green  Skills  &  AI  Foundation  Course ‚Äì VTU,\n",
    "AICTE,  Shell India Studied  AI  for  sustainability,\n",
    "smart energy, and ethical tech innovation.\n",
    "    \"\"\"\n",
    "]\n",
    "for i, resume in enumerate(test_resumes, 1):\n",
    "    print(f\"\\n--- Resume {i} ---\")\n",
    "    personal_info = extract_personal_info(resume)\n",
    "    known_skills_found, unknown_skills_found = extract_skills(resume)\n",
    "    print(\"üßë Personal Info:\", personal_info)\n",
    "    print(\"‚úÖ Known Skills:\", known_skills_found)\n",
    "    print(\"‚ùì Unknown Skills:\", unknown_skills_found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c3e7b-7600-4bac-8d61-ba72310ca289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3e4a7-1029-401d-be89-3bfec73378b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1150e8-a166-4bef-a7b8-ae08c829996e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2236fd-5713-49c9-8fa6-bf3de26f91cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24304ae1-71a5-4581-af62-7bd8ab6c4344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdcbeaa-1480-43ec-ae0d-aa7835497ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16936c9-88c7-4713-9863-a3521ab9c5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535dad00-9439-433a-be34-21090f962fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8da032-8ff8-4de6-838f-d5db70c2c4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025fa18d-84ef-4908-8f13-75dc5d10eb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "412f3362-cf16-4b7a-82ec-a68972406fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 ‚Äî Losses: {'ner': 2.793452283635041}\n",
      "Epoch 2/40 ‚Äî Losses: {'ner': 1.4955650032058458}\n",
      "Epoch 3/40 ‚Äî Losses: {'ner': 0.00016181467963910338}\n",
      "Epoch 4/40 ‚Äî Losses: {'ner': 3.6381758325934236e-05}\n",
      "Epoch 5/40 ‚Äî Losses: {'ner': 8.342458645778399e-09}\n",
      "Epoch 6/40 ‚Äî Losses: {'ner': 1.0167777211994722e-08}\n",
      "Epoch 7/40 ‚Äî Losses: {'ner': 1.058194665469815e-05}\n",
      "Epoch 8/40 ‚Äî Losses: {'ner': 3.101190213313754e-06}\n",
      "Epoch 9/40 ‚Äî Losses: {'ner': 5.486041816539672e-09}\n",
      "Epoch 10/40 ‚Äî Losses: {'ner': 9.128216009161061e-11}\n",
      "Epoch 11/40 ‚Äî Losses: {'ner': 2.7164852514602936e-11}\n",
      "Epoch 12/40 ‚Äî Losses: {'ner': 6.693852608417624e-11}\n",
      "Epoch 13/40 ‚Äî Losses: {'ner': 5.778224264175894e-10}\n",
      "Epoch 14/40 ‚Äî Losses: {'ner': 1.9530703547548156e-11}\n",
      "Epoch 15/40 ‚Äî Losses: {'ner': 4.339097283356816e-11}\n",
      "Epoch 16/40 ‚Äî Losses: {'ner': 3.550857759234886e-12}\n",
      "Epoch 17/40 ‚Äî Losses: {'ner': 5.1336386891847157e-14}\n",
      "Epoch 18/40 ‚Äî Losses: {'ner': 2.333377875870449e-11}\n",
      "Epoch 19/40 ‚Äî Losses: {'ner': 4.942122774962182e-12}\n",
      "Epoch 20/40 ‚Äî Losses: {'ner': 1.6352313025208685e-07}\n",
      "Epoch 21/40 ‚Äî Losses: {'ner': 2.0825548628695694e-12}\n",
      "Epoch 22/40 ‚Äî Losses: {'ner': 8.866116339635167e-11}\n",
      "Epoch 23/40 ‚Äî Losses: {'ner': 2.8187598029267567e-13}\n",
      "Epoch 24/40 ‚Äî Losses: {'ner': 3.836249245379736e-11}\n",
      "Epoch 25/40 ‚Äî Losses: {'ner': 3.617346751101295e-12}\n",
      "Epoch 26/40 ‚Äî Losses: {'ner': 2.560347660144014e-12}\n",
      "Epoch 27/40 ‚Äî Losses: {'ner': 2.313627783427976e-12}\n",
      "Epoch 28/40 ‚Äî Losses: {'ner': 2.0752974808686616e-11}\n",
      "Epoch 29/40 ‚Äî Losses: {'ner': 8.162408403332538e-13}\n",
      "Epoch 30/40 ‚Äî Losses: {'ner': 6.234041033933253e-12}\n",
      "Epoch 31/40 ‚Äî Losses: {'ner': 1.281545931392905e-12}\n",
      "Epoch 32/40 ‚Äî Losses: {'ner': 1.0867522905010025e-12}\n",
      "Epoch 33/40 ‚Äî Losses: {'ner': 3.027717675061977e-12}\n",
      "Epoch 34/40 ‚Äî Losses: {'ner': 3.5145155046017354e-11}\n",
      "Epoch 35/40 ‚Äî Losses: {'ner': 2.0549413910000404e-12}\n",
      "Epoch 36/40 ‚Äî Losses: {'ner': 3.97940161874271e-12}\n",
      "Epoch 37/40 ‚Äî Losses: {'ner': 4.274092551428929e-12}\n",
      "Epoch 38/40 ‚Äî Losses: {'ner': 6.927862079593775e-11}\n",
      "Epoch 39/40 ‚Äî Losses: {'ner': 6.64659677684227e-12}\n",
      "Epoch 40/40 ‚Äî Losses: {'ner': 2.0332198610360248e-10}\n",
      "Model saved to ner_model/\n",
      "Precision: 0.00%, Recall: 0.00%, F1-score: 0.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "import re\n",
    "\n",
    "# -------------------------------\n",
    "# 1Ô∏è‚É£ Skill variations\n",
    "# -------------------------------\n",
    "skill_variations = {\n",
    "    \"node.js\": [\"node.js\", \"nodejs\", \"nodeJs\", \"NODE.JS\"],\n",
    "    \"react.js\": [\"react.js\", \"reactjs\", \"reactJs\", \"React\"],\n",
    "    \"express.js\": [\"express.js\", \"express\", \"Express.js\"],\n",
    "    \"mysql\": [\"mysql\", \"MySQL\"],\n",
    "    \"mongodb\": [\"mongodb\", \"mongoDB\", \"MongoDb\"],\n",
    "    \"vite\": [\"vite\", \"VITE\"],\n",
    "    \"jwt\": [\"jwt\", \"JWT\"],\n",
    "    \"postman\": [\"postman\", \"POSTMAN\"],\n",
    "    \"aws lambda\": [\"aws lambda\", \"AWS Lambda\", \"Lambda\"],\n",
    "    \"aws cognito\": [\"aws cognito\", \"Cognito\"],\n",
    "    \"dynamodb\": [\"dynamodb\", \"DynamoDB\"],\n",
    "    \"python\": [\"python\", \"Python\"],\n",
    "    \"spacy\": [\"spacy\", \"spaCy\"],\n",
    "    \"librosa\": [\"librosa\", \"Librosa\"],\n",
    "    \"bootstrap\": [\"bootstrap\", \"Bootstrap\"],\n",
    "    \"jsx\": [\"jsx\", \"JSX\"],\n",
    "    \"rest api\": [\"rest api\", \"REST API\"],\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 2Ô∏è‚É£ Load SpaCy and add NER\n",
    "# -------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "if \"SKILL\" not in ner.labels:\n",
    "    ner.add_label(\"SKILL\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ Text cleaning\n",
    "# -------------------------------\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"[\\(\\)|,]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.lower()\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ Auto-generate TRAIN_DATA from resumes\n",
    "# -------------------------------\n",
    "def generate_train_data(resumes, skill_variations):\n",
    "    train_data = []\n",
    "    for resume in resumes:\n",
    "        text_original = resume\n",
    "        entities = []\n",
    "        text_lower = text_original.lower()\n",
    "        for skill, variations in skill_variations.items():\n",
    "            for var in variations:\n",
    "                for match in re.finditer(r'\\b' + re.escape(var.lower()) + r'\\b', text_lower):\n",
    "                    entities.append((match.start(), match.end(), \"SKILL\"))\n",
    "        train_data.append((text_original, {\"entities\": entities}))\n",
    "    return train_data\n",
    "\n",
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ PhraseMatcher for known skills\n",
    "# -------------------------------\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "for skill, variations in skill_variations.items():\n",
    "    patterns = [nlp.make_doc(v) for v in variations]\n",
    "    matcher.add(skill, patterns)\n",
    "\n",
    "# -------------------------------\n",
    "# 6Ô∏è‚É£ Skill extraction\n",
    "# -------------------------------\n",
    "def extract_skills(text):\n",
    "    text_clean = clean_text(text)\n",
    "    doc = nlp(text_clean)\n",
    "\n",
    "    matched_skills = set()\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        skill_label = nlp.vocab.strings[match_id]\n",
    "        matched_skills.add(skill_label.lower())\n",
    "\n",
    "    ner_skills = set([ent.text.lower() for ent in doc.ents if ent.label_ == \"SKILL\"])\n",
    "    all_skills = matched_skills.union(ner_skills)\n",
    "\n",
    "    known = sorted([s for s in all_skills if s in skill_variations])\n",
    "    unknown = sorted([s for s in all_skills if s not in skill_variations])\n",
    "    return known, unknown\n",
    "\n",
    "# -------------------------------\n",
    "# 7Ô∏è‚É£ Personal info extraction\n",
    "# -------------------------------\n",
    "def extract_personal_info(text):\n",
    "    lines = text.splitlines()\n",
    "    name = \"\"\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and re.match(r\"^[A-Za-z\\s\\-\\.]+$\", line):\n",
    "            name = line\n",
    "            break\n",
    "    email = re.search(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "    phone = re.search(r\"(\\+?\\d[\\d\\s-]{7,}\\d)\", text)\n",
    "    linkedin = re.search(r\"https?://(www\\.)?linkedin\\.com/[^\\s,]+\", text)\n",
    "    github = re.search(r\"https?://(www\\.)?github\\.com/[^\\s,]+\", text)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"email\": email.group(0) if email else \"\",\n",
    "        \"phone\": phone.group(0) if phone else \"\",\n",
    "        \"linkedin\": linkedin.group(0) if linkedin else \"\",\n",
    "        \"github\": github.group(0) if github else \"\"\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 8Ô∏è‚É£ Validation function to check overfitting/underfitting\n",
    "# -------------------------------\n",
    "def validate_ner(resumes):\n",
    "    correct, predicted_total, actual_total = 0, 0, 0\n",
    "    for resume in resumes:\n",
    "        doc = nlp(resume)\n",
    "        predicted = set([ent.text.lower() for ent in doc.ents if ent.label_ == \"SKILL\"])\n",
    "        actual = set()\n",
    "        for skill, variations in skill_variations.items():\n",
    "            for var in variations:\n",
    "                if re.search(r'\\b' + re.escape(var) + r'\\b', resume, re.IGNORECASE):\n",
    "                    actual.add(skill)\n",
    "        correct += len(predicted & actual)\n",
    "        predicted_total += len(predicted)\n",
    "        actual_total += len(actual)\n",
    "    precision = correct / predicted_total if predicted_total else 0\n",
    "    recall = correct / actual_total if actual_total else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    print(f\"Precision: {precision:.2%}, Recall: {recall:.2%}, F1-score: {f1:.2%}\")\n",
    "    return precision, recall, f1\n",
    "\n",
    "# -------------------------------\n",
    "# 9Ô∏è‚É£ Train NER\n",
    "# -------------------------------\n",
    "def train_ner(resumes, skill_variations, n_epochs=30):\n",
    "    TRAIN_DATA = generate_train_data(resumes, skill_variations)\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.resume_training()\n",
    "        for epoch in range(n_epochs):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(2.0, 16.0, 1.5))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                examples = [Example.from_dict(nlp.make_doc(t), a) for t, a in batch]\n",
    "                nlp.update(examples, sgd=optimizer, drop=0.2, losses=losses)\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} ‚Äî Losses: {losses}\")\n",
    "\n",
    "    nlp.to_disk(\"ner_model\")\n",
    "    print(\"Model saved to ner_model/\")\n",
    "\n",
    "# -------------------------------\n",
    "#  üîü Example usage\n",
    "# -------------------------------\n",
    "train_resumes = [\n",
    "    \"\"\"ALICE JOHNSON ...\"\"\",\n",
    "    \"\"\"BOB SMITH ...\"\"\",\n",
    "    \"\"\"CAROL LEE ...\"\"\",\n",
    "    \"\"\"DAVID KUMAR ...\"\"\",\n",
    "    \"\"\"EVELYN CHEN ...\"\"\",\n",
    "    \"\"\"FRANK WU ...\"\"\"\n",
    "]\n",
    "\n",
    "train_ner(train_resumes, skill_variations, n_epochs=40)\n",
    "validate_ner(train_resumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4a161-f8c3-4166-9117-9d393195bb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9af0d9-5eb0-4ca3-9464-982ff858b8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7a4c3-6d44-4e7c-9241-03adb85eebd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48af7ab5-6e68-480e-a98b-87740e17f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "     ---------------------------------------- 0.0/400.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.3/400.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.0/400.7 MB 3.0 MB/s eta 0:02:16\n",
      "     ---------------------------------------- 1.6/400.7 MB 3.1 MB/s eta 0:02:09\n",
      "     ---------------------------------------- 2.9/400.7 MB 3.8 MB/s eta 0:01:45\n",
      "     ---------------------------------------- 3.9/400.7 MB 4.1 MB/s eta 0:01:37\n",
      "     ---------------------------------------- 4.7/400.7 MB 4.1 MB/s eta 0:01:38\n",
      "      --------------------------------------- 5.8/400.7 MB 4.2 MB/s eta 0:01:35\n",
      "      --------------------------------------- 6.8/400.7 MB 4.4 MB/s eta 0:01:31\n",
      "      --------------------------------------- 7.9/400.7 MB 4.5 MB/s eta 0:01:29\n",
      "      --------------------------------------- 9.2/400.7 MB 4.6 MB/s eta 0:01:26\n",
      "      -------------------------------------- 10.0/400.7 MB 4.6 MB/s eta 0:01:25\n",
      "     - ------------------------------------- 11.0/400.7 MB 4.6 MB/s eta 0:01:24\n",
      "     - ------------------------------------- 12.1/400.7 MB 4.7 MB/s eta 0:01:23\n",
      "     - ------------------------------------- 13.1/400.7 MB 4.7 MB/s eta 0:01:23\n",
      "     - ------------------------------------- 14.2/400.7 MB 4.7 MB/s eta 0:01:23\n",
      "     - ------------------------------------- 15.5/400.7 MB 4.8 MB/s eta 0:01:20\n",
      "     - ------------------------------------- 16.8/400.7 MB 4.9 MB/s eta 0:01:19\n",
      "     - ------------------------------------- 17.6/400.7 MB 4.9 MB/s eta 0:01:19\n",
      "     - ------------------------------------- 18.9/400.7 MB 4.9 MB/s eta 0:01:18\n",
      "     - ------------------------------------- 19.9/400.7 MB 4.9 MB/s eta 0:01:18\n",
      "     -- ------------------------------------ 20.7/400.7 MB 4.9 MB/s eta 0:01:18\n",
      "     -- ------------------------------------ 21.8/400.7 MB 4.9 MB/s eta 0:01:18\n",
      "     -- ------------------------------------ 22.8/400.7 MB 4.9 MB/s eta 0:01:17\n",
      "     -- ------------------------------------ 24.1/400.7 MB 5.0 MB/s eta 0:01:16\n",
      "     -- ------------------------------------ 24.9/400.7 MB 4.9 MB/s eta 0:01:17\n",
      "     -- ------------------------------------ 26.2/400.7 MB 5.0 MB/s eta 0:01:16\n",
      "     -- ------------------------------------ 27.5/400.7 MB 5.0 MB/s eta 0:01:15\n",
      "     -- ------------------------------------ 28.3/400.7 MB 5.0 MB/s eta 0:01:15\n",
      "     -- ------------------------------------ 29.1/400.7 MB 5.0 MB/s eta 0:01:15\n",
      "     -- ------------------------------------ 29.9/400.7 MB 4.9 MB/s eta 0:01:16\n",
      "     -- ------------------------------------ 30.7/400.7 MB 4.9 MB/s eta 0:01:17\n",
      "     --- ----------------------------------- 31.5/400.7 MB 4.8 MB/s eta 0:01:17\n",
      "     --- ----------------------------------- 32.0/400.7 MB 4.8 MB/s eta 0:01:18\n",
      "     --- ----------------------------------- 32.8/400.7 MB 4.7 MB/s eta 0:01:18\n",
      "     --- ----------------------------------- 33.3/400.7 MB 4.7 MB/s eta 0:01:18\n",
      "     --- ----------------------------------- 33.8/400.7 MB 4.6 MB/s eta 0:01:20\n",
      "     --- ----------------------------------- 34.6/400.7 MB 4.6 MB/s eta 0:01:20\n",
      "     --- ----------------------------------- 35.1/400.7 MB 4.6 MB/s eta 0:01:21\n",
      "     --- ----------------------------------- 35.9/400.7 MB 4.5 MB/s eta 0:01:21\n",
      "     --- ----------------------------------- 36.7/400.7 MB 4.5 MB/s eta 0:01:22\n",
      "     --- ----------------------------------- 37.5/400.7 MB 4.5 MB/s eta 0:01:22\n",
      "     --- ----------------------------------- 38.0/400.7 MB 4.4 MB/s eta 0:01:22\n",
      "     --- ----------------------------------- 38.8/400.7 MB 4.4 MB/s eta 0:01:22\n",
      "     --- ----------------------------------- 39.8/400.7 MB 4.4 MB/s eta 0:01:22\n",
      "     --- ----------------------------------- 40.4/400.7 MB 4.4 MB/s eta 0:01:23\n",
      "     ---- ---------------------------------- 41.4/400.7 MB 4.4 MB/s eta 0:01:23\n",
      "     ---- ---------------------------------- 42.2/400.7 MB 4.4 MB/s eta 0:01:22\n",
      "     ---- ---------------------------------- 42.7/400.7 MB 4.3 MB/s eta 0:01:23\n",
      "     ---- ---------------------------------- 43.5/400.7 MB 4.3 MB/s eta 0:01:23\n",
      "     ---- ---------------------------------- 44.3/400.7 MB 4.3 MB/s eta 0:01:23\n",
      "     ---- ---------------------------------- 45.1/400.7 MB 4.3 MB/s eta 0:01:23\n",
      "     ---- ---------------------------------- 45.9/400.7 MB 4.3 MB/s eta 0:01:23\n",
      "     ---- ---------------------------------- 46.7/400.7 MB 4.3 MB/s eta 0:01:23\n",
      "     ---- ---------------------------------- 47.2/400.7 MB 4.3 MB/s eta 0:01:23\n",
      "     ---- ---------------------------------- 48.0/400.7 MB 4.2 MB/s eta 0:01:24\n",
      "     ---- ---------------------------------- 48.8/400.7 MB 4.2 MB/s eta 0:01:24\n",
      "     ---- ---------------------------------- 49.5/400.7 MB 4.2 MB/s eta 0:01:24\n",
      "     ---- ---------------------------------- 50.9/400.7 MB 4.3 MB/s eta 0:01:23\n",
      "     ----- --------------------------------- 52.2/400.7 MB 4.3 MB/s eta 0:01:22\n",
      "     ----- --------------------------------- 53.0/400.7 MB 4.3 MB/s eta 0:01:22\n",
      "     ----- --------------------------------- 54.3/400.7 MB 4.3 MB/s eta 0:01:21\n",
      "     ----- --------------------------------- 54.8/400.7 MB 4.3 MB/s eta 0:01:21\n",
      "     ----- --------------------------------- 55.3/400.7 MB 4.3 MB/s eta 0:01:22\n",
      "     ----- --------------------------------- 55.8/400.7 MB 4.2 MB/s eta 0:01:22\n",
      "     ----- --------------------------------- 56.6/400.7 MB 4.2 MB/s eta 0:01:22\n",
      "     ----- --------------------------------- 57.1/400.7 MB 4.2 MB/s eta 0:01:22\n",
      "     ----- --------------------------------- 58.2/400.7 MB 4.2 MB/s eta 0:01:22\n",
      "     ----- --------------------------------- 59.0/400.7 MB 4.2 MB/s eta 0:01:22\n",
      "     ----- --------------------------------- 60.0/400.7 MB 4.2 MB/s eta 0:01:21\n",
      "     ----- --------------------------------- 61.1/400.7 MB 4.2 MB/s eta 0:01:21\n",
      "     ------ -------------------------------- 62.1/400.7 MB 4.2 MB/s eta 0:01:20\n",
      "     ------ -------------------------------- 63.2/400.7 MB 4.2 MB/s eta 0:01:20\n",
      "     ------ -------------------------------- 64.2/400.7 MB 4.3 MB/s eta 0:01:19\n",
      "     ------ -------------------------------- 65.3/400.7 MB 4.3 MB/s eta 0:01:19\n",
      "     ------ -------------------------------- 66.6/400.7 MB 4.3 MB/s eta 0:01:18\n",
      "     ------ -------------------------------- 67.6/400.7 MB 4.3 MB/s eta 0:01:18\n",
      "     ------ -------------------------------- 68.7/400.7 MB 4.3 MB/s eta 0:01:17\n",
      "     ------ -------------------------------- 69.7/400.7 MB 4.3 MB/s eta 0:01:17\n",
      "     ------ -------------------------------- 70.5/400.7 MB 4.3 MB/s eta 0:01:17\n",
      "     ------ -------------------------------- 71.6/400.7 MB 4.3 MB/s eta 0:01:17\n",
      "     ------- ------------------------------- 72.6/400.7 MB 4.3 MB/s eta 0:01:16\n",
      "     ------- ------------------------------- 73.7/400.7 MB 4.3 MB/s eta 0:01:16\n",
      "     ------- ------------------------------- 74.2/400.7 MB 4.3 MB/s eta 0:01:16\n",
      "     ------- ------------------------------- 75.0/400.7 MB 4.3 MB/s eta 0:01:16\n",
      "     ------- ------------------------------- 75.5/400.7 MB 4.3 MB/s eta 0:01:16\n",
      "     ------- ------------------------------- 76.8/400.7 MB 4.3 MB/s eta 0:01:16\n",
      "     ------- ------------------------------- 77.9/400.7 MB 4.3 MB/s eta 0:01:15\n",
      "     ------- ------------------------------- 78.9/400.7 MB 4.3 MB/s eta 0:01:15\n",
      "     ------- ------------------------------- 80.0/400.7 MB 4.3 MB/s eta 0:01:14\n",
      "     ------- ------------------------------- 81.3/400.7 MB 4.4 MB/s eta 0:01:14\n",
      "     -------- ------------------------------ 82.3/400.7 MB 4.4 MB/s eta 0:01:13\n",
      "     -------- ------------------------------ 83.4/400.7 MB 4.4 MB/s eta 0:01:13\n",
      "     -------- ------------------------------ 84.7/400.7 MB 4.4 MB/s eta 0:01:12\n",
      "     -------- ------------------------------ 85.7/400.7 MB 4.4 MB/s eta 0:01:12\n",
      "     -------- ------------------------------ 86.5/400.7 MB 4.4 MB/s eta 0:01:12\n",
      "     -------- ------------------------------ 87.6/400.7 MB 4.4 MB/s eta 0:01:12\n",
      "     -------- ------------------------------ 88.9/400.7 MB 4.4 MB/s eta 0:01:11\n",
      "     -------- ------------------------------ 89.9/400.7 MB 4.4 MB/s eta 0:01:11\n",
      "     -------- ------------------------------ 91.0/400.7 MB 4.4 MB/s eta 0:01:10\n",
      "     -------- ------------------------------ 91.5/400.7 MB 4.4 MB/s eta 0:01:10\n",
      "     --------- ----------------------------- 92.5/400.7 MB 4.4 MB/s eta 0:01:10\n",
      "     --------- ----------------------------- 93.3/400.7 MB 4.4 MB/s eta 0:01:10\n",
      "     --------- ----------------------------- 94.4/400.7 MB 4.4 MB/s eta 0:01:10\n",
      "     --------- ----------------------------- 95.7/400.7 MB 4.4 MB/s eta 0:01:09\n",
      "     --------- ----------------------------- 96.7/400.7 MB 4.4 MB/s eta 0:01:09\n",
      "     --------- ----------------------------- 97.8/400.7 MB 4.4 MB/s eta 0:01:09\n",
      "     --------- ----------------------------- 98.6/400.7 MB 4.4 MB/s eta 0:01:08\n",
      "     --------- ----------------------------- 99.9/400.7 MB 4.4 MB/s eta 0:01:08\n",
      "     --------- ---------------------------- 100.9/400.7 MB 4.4 MB/s eta 0:01:08\n",
      "     --------- ---------------------------- 102.0/400.7 MB 4.5 MB/s eta 0:01:07\n",
      "     --------- ---------------------------- 103.3/400.7 MB 4.5 MB/s eta 0:01:07\n",
      "     --------- ---------------------------- 104.3/400.7 MB 4.5 MB/s eta 0:01:07\n",
      "     --------- ---------------------------- 105.4/400.7 MB 4.5 MB/s eta 0:01:06\n",
      "     ---------- --------------------------- 106.4/400.7 MB 4.5 MB/s eta 0:01:06\n",
      "     ---------- --------------------------- 107.7/400.7 MB 4.5 MB/s eta 0:01:06\n",
      "     ---------- --------------------------- 108.3/400.7 MB 4.5 MB/s eta 0:01:05\n",
      "     ---------- --------------------------- 109.1/400.7 MB 4.5 MB/s eta 0:01:06\n",
      "     ---------- --------------------------- 110.1/400.7 MB 4.5 MB/s eta 0:01:05\n",
      "     ---------- --------------------------- 111.1/400.7 MB 4.5 MB/s eta 0:01:05\n",
      "     ---------- --------------------------- 111.9/400.7 MB 4.5 MB/s eta 0:01:05\n",
      "     ---------- --------------------------- 113.2/400.7 MB 4.5 MB/s eta 0:01:04\n",
      "     ---------- --------------------------- 114.3/400.7 MB 4.5 MB/s eta 0:01:04\n",
      "     ---------- --------------------------- 115.1/400.7 MB 4.5 MB/s eta 0:01:04\n",
      "     ----------- -------------------------- 116.1/400.7 MB 4.5 MB/s eta 0:01:04\n",
      "     ----------- -------------------------- 117.4/400.7 MB 4.5 MB/s eta 0:01:03\n",
      "     ----------- -------------------------- 118.8/400.7 MB 4.5 MB/s eta 0:01:03\n",
      "     ----------- -------------------------- 119.5/400.7 MB 4.5 MB/s eta 0:01:03\n",
      "     ----------- -------------------------- 120.3/400.7 MB 4.5 MB/s eta 0:01:02\n",
      "     ----------- -------------------------- 121.4/400.7 MB 4.5 MB/s eta 0:01:02\n",
      "     ----------- -------------------------- 122.4/400.7 MB 4.5 MB/s eta 0:01:02\n",
      "     ----------- -------------------------- 123.7/400.7 MB 4.5 MB/s eta 0:01:02\n",
      "     ----------- -------------------------- 124.8/400.7 MB 4.5 MB/s eta 0:01:01\n",
      "     ----------- -------------------------- 125.8/400.7 MB 4.6 MB/s eta 0:01:01\n",
      "     ------------ ------------------------- 126.9/400.7 MB 4.5 MB/s eta 0:01:01\n",
      "     ------------ ------------------------- 128.2/400.7 MB 4.6 MB/s eta 0:01:00\n",
      "     ------------ ------------------------- 129.5/400.7 MB 4.6 MB/s eta 0:01:00\n",
      "     ------------ ------------------------- 130.5/400.7 MB 4.6 MB/s eta 0:00:59\n",
      "     ------------ ------------------------- 131.6/400.7 MB 4.6 MB/s eta 0:00:59\n",
      "     ------------ ------------------------- 132.4/400.7 MB 4.6 MB/s eta 0:00:59\n",
      "     ------------ ------------------------- 133.4/400.7 MB 4.6 MB/s eta 0:00:59\n",
      "     ------------ ------------------------- 134.5/400.7 MB 4.6 MB/s eta 0:00:59\n",
      "     ------------ ------------------------- 135.8/400.7 MB 4.6 MB/s eta 0:00:58\n",
      "     ------------- ------------------------ 137.1/400.7 MB 4.6 MB/s eta 0:00:58\n",
      "     ------------- ------------------------ 138.1/400.7 MB 4.6 MB/s eta 0:00:57\n",
      "     ------------- ------------------------ 139.2/400.7 MB 4.6 MB/s eta 0:00:57\n",
      "     ------------- ------------------------ 140.5/400.7 MB 4.6 MB/s eta 0:00:56\n",
      "     ------------- ------------------------ 141.6/400.7 MB 4.7 MB/s eta 0:00:56\n",
      "     ------------- ------------------------ 142.3/400.7 MB 4.6 MB/s eta 0:00:56\n",
      "     ------------- ------------------------ 143.7/400.7 MB 4.7 MB/s eta 0:00:56\n",
      "     ------------- ------------------------ 145.0/400.7 MB 4.7 MB/s eta 0:00:55\n",
      "     ------------- ------------------------ 146.0/400.7 MB 4.7 MB/s eta 0:00:55\n",
      "     ------------- ------------------------ 146.8/400.7 MB 4.7 MB/s eta 0:00:55\n",
      "     -------------- ----------------------- 148.1/400.7 MB 4.7 MB/s eta 0:00:55\n",
      "     -------------- ----------------------- 148.9/400.7 MB 4.7 MB/s eta 0:00:55\n",
      "     -------------- ----------------------- 149.9/400.7 MB 4.6 MB/s eta 0:00:54\n",
      "     -------------- ----------------------- 151.3/400.7 MB 4.7 MB/s eta 0:00:54\n",
      "     -------------- ----------------------- 152.3/400.7 MB 4.7 MB/s eta 0:00:54\n",
      "     -------------- ----------------------- 153.1/400.7 MB 4.7 MB/s eta 0:00:54\n",
      "     -------------- ----------------------- 154.4/400.7 MB 4.7 MB/s eta 0:00:53\n",
      "     -------------- ----------------------- 155.7/400.7 MB 4.6 MB/s eta 0:00:53\n",
      "     -------------- ----------------------- 156.8/400.7 MB 4.7 MB/s eta 0:00:53\n",
      "     -------------- ----------------------- 157.8/400.7 MB 4.7 MB/s eta 0:00:53\n",
      "     --------------- ---------------------- 159.1/400.7 MB 4.7 MB/s eta 0:00:52\n",
      "     --------------- ---------------------- 160.4/400.7 MB 4.7 MB/s eta 0:00:52\n",
      "     --------------- ---------------------- 161.2/400.7 MB 4.7 MB/s eta 0:00:52\n",
      "     --------------- ---------------------- 162.5/400.7 MB 4.7 MB/s eta 0:00:51\n",
      "     --------------- ---------------------- 163.8/400.7 MB 4.7 MB/s eta 0:00:51\n",
      "     --------------- ---------------------- 164.6/400.7 MB 4.7 MB/s eta 0:00:51\n",
      "     --------------- ---------------------- 165.9/400.7 MB 4.7 MB/s eta 0:00:51\n",
      "     --------------- ---------------------- 167.2/400.7 MB 4.7 MB/s eta 0:00:51\n",
      "     --------------- ---------------------- 168.6/400.7 MB 4.7 MB/s eta 0:00:50\n",
      "     ---------------- --------------------- 169.3/400.7 MB 4.7 MB/s eta 0:00:50\n",
      "     ---------------- --------------------- 170.4/400.7 MB 4.7 MB/s eta 0:00:50\n",
      "     ---------------- --------------------- 171.7/400.7 MB 4.7 MB/s eta 0:00:49\n",
      "     ---------------- --------------------- 172.8/400.7 MB 4.7 MB/s eta 0:00:49\n",
      "     ---------------- --------------------- 173.8/400.7 MB 4.7 MB/s eta 0:00:48\n",
      "     ---------------- --------------------- 175.1/400.7 MB 4.8 MB/s eta 0:00:48\n",
      "     ---------------- --------------------- 176.4/400.7 MB 4.8 MB/s eta 0:00:47\n",
      "     ---------------- --------------------- 177.2/400.7 MB 4.8 MB/s eta 0:00:47\n",
      "     ---------------- --------------------- 178.5/400.7 MB 4.8 MB/s eta 0:00:47\n",
      "     ----------------- -------------------- 179.8/400.7 MB 4.8 MB/s eta 0:00:46\n",
      "     ----------------- -------------------- 180.9/400.7 MB 4.8 MB/s eta 0:00:46\n",
      "     ----------------- -------------------- 182.2/400.7 MB 4.9 MB/s eta 0:00:45\n",
      "     ----------------- -------------------- 183.2/400.7 MB 4.9 MB/s eta 0:00:45\n",
      "     ----------------- -------------------- 184.3/400.7 MB 4.9 MB/s eta 0:00:45\n",
      "     ----------------- -------------------- 185.3/400.7 MB 4.9 MB/s eta 0:00:45\n",
      "     ----------------- -------------------- 186.6/400.7 MB 4.9 MB/s eta 0:00:44\n",
      "     ----------------- -------------------- 188.0/400.7 MB 4.9 MB/s eta 0:00:44\n",
      "     ----------------- -------------------- 188.7/400.7 MB 4.9 MB/s eta 0:00:43\n",
      "     ------------------ ------------------- 189.8/400.7 MB 4.9 MB/s eta 0:00:43\n",
      "     ------------------ ------------------- 190.8/400.7 MB 4.9 MB/s eta 0:00:43\n",
      "     ------------------ ------------------- 191.9/400.7 MB 5.0 MB/s eta 0:00:43\n",
      "     ------------------ ------------------- 192.9/400.7 MB 5.0 MB/s eta 0:00:42\n",
      "     ------------------ ------------------- 194.2/400.7 MB 5.0 MB/s eta 0:00:42\n",
      "     ------------------ ------------------- 195.6/400.7 MB 5.0 MB/s eta 0:00:42\n",
      "     ------------------ ------------------- 196.1/400.7 MB 5.0 MB/s eta 0:00:42\n",
      "     ------------------ ------------------- 197.1/400.7 MB 5.0 MB/s eta 0:00:41\n",
      "     ------------------ ------------------- 198.4/400.7 MB 5.0 MB/s eta 0:00:41\n",
      "     ------------------ ------------------- 199.8/400.7 MB 5.0 MB/s eta 0:00:40\n",
      "     ------------------- ------------------ 200.5/400.7 MB 5.0 MB/s eta 0:00:40\n",
      "     ------------------- ------------------ 201.9/400.7 MB 5.0 MB/s eta 0:00:40\n",
      "     ------------------- ------------------ 202.9/400.7 MB 5.0 MB/s eta 0:00:40\n",
      "     ------------------- ------------------ 204.2/400.7 MB 5.1 MB/s eta 0:00:39\n",
      "     ------------------- ------------------ 205.0/400.7 MB 5.0 MB/s eta 0:00:39\n",
      "     ------------------- ------------------ 206.3/400.7 MB 5.1 MB/s eta 0:00:39\n",
      "     ------------------- ------------------ 207.4/400.7 MB 5.1 MB/s eta 0:00:39\n",
      "     ------------------- ------------------ 208.4/400.7 MB 5.1 MB/s eta 0:00:38\n",
      "     ------------------- ------------------ 209.5/400.7 MB 5.1 MB/s eta 0:00:38\n",
      "     ------------------- ------------------ 210.8/400.7 MB 5.1 MB/s eta 0:00:38\n",
      "     -------------------- ----------------- 211.8/400.7 MB 5.1 MB/s eta 0:00:37\n",
      "     -------------------- ----------------- 213.1/400.7 MB 5.1 MB/s eta 0:00:37\n",
      "     -------------------- ----------------- 214.4/400.7 MB 5.2 MB/s eta 0:00:37\n",
      "     -------------------- ----------------- 215.7/400.7 MB 5.2 MB/s eta 0:00:36\n",
      "     -------------------- ----------------- 216.8/400.7 MB 5.2 MB/s eta 0:00:36\n",
      "     -------------------- ----------------- 218.1/400.7 MB 5.2 MB/s eta 0:00:36\n",
      "     -------------------- ----------------- 219.4/400.7 MB 5.2 MB/s eta 0:00:35\n",
      "     -------------------- ----------------- 220.2/400.7 MB 5.2 MB/s eta 0:00:35\n",
      "     -------------------- ----------------- 221.2/400.7 MB 5.2 MB/s eta 0:00:35\n",
      "     --------------------- ---------------- 222.6/400.7 MB 5.2 MB/s eta 0:00:35\n",
      "     --------------------- ---------------- 223.9/400.7 MB 5.2 MB/s eta 0:00:35\n",
      "     --------------------- ---------------- 224.9/400.7 MB 5.2 MB/s eta 0:00:34\n",
      "     --------------------- ---------------- 226.0/400.7 MB 5.2 MB/s eta 0:00:34\n",
      "     --------------------- ---------------- 227.3/400.7 MB 5.2 MB/s eta 0:00:34\n",
      "     --------------------- ---------------- 228.6/400.7 MB 5.2 MB/s eta 0:00:33\n",
      "     --------------------- ---------------- 229.6/400.7 MB 5.2 MB/s eta 0:00:33\n",
      "     --------------------- ---------------- 230.9/400.7 MB 5.2 MB/s eta 0:00:33\n",
      "     ---------------------- --------------- 232.0/400.7 MB 5.3 MB/s eta 0:00:33\n",
      "     ---------------------- --------------- 232.8/400.7 MB 5.3 MB/s eta 0:00:32\n",
      "     ---------------------- --------------- 234.1/400.7 MB 5.3 MB/s eta 0:00:32\n",
      "     ---------------------- --------------- 235.4/400.7 MB 5.3 MB/s eta 0:00:32\n",
      "     ---------------------- --------------- 236.5/400.7 MB 5.3 MB/s eta 0:00:32\n",
      "     ---------------------- --------------- 237.8/400.7 MB 5.3 MB/s eta 0:00:31\n",
      "     ---------------------- --------------- 239.1/400.7 MB 5.3 MB/s eta 0:00:31\n",
      "     ---------------------- --------------- 240.4/400.7 MB 5.3 MB/s eta 0:00:31\n",
      "     ---------------------- --------------- 241.4/400.7 MB 5.3 MB/s eta 0:00:31\n",
      "     ----------------------- -------------- 242.7/400.7 MB 5.3 MB/s eta 0:00:30\n",
      "     ----------------------- -------------- 244.1/400.7 MB 5.3 MB/s eta 0:00:30\n",
      "     ----------------------- -------------- 245.4/400.7 MB 5.3 MB/s eta 0:00:30\n",
      "     ----------------------- -------------- 246.4/400.7 MB 5.3 MB/s eta 0:00:30\n",
      "     ----------------------- -------------- 248.0/400.7 MB 5.3 MB/s eta 0:00:29\n",
      "     ----------------------- -------------- 249.3/400.7 MB 5.3 MB/s eta 0:00:29\n",
      "     ----------------------- -------------- 250.3/400.7 MB 5.3 MB/s eta 0:00:29\n",
      "     ----------------------- -------------- 251.7/400.7 MB 5.4 MB/s eta 0:00:28\n",
      "     ----------------------- -------------- 253.0/400.7 MB 5.4 MB/s eta 0:00:28\n",
      "     ------------------------ ------------- 254.0/400.7 MB 5.4 MB/s eta 0:00:28\n",
      "     ------------------------ ------------- 255.1/400.7 MB 5.4 MB/s eta 0:00:28\n",
      "     ------------------------ ------------- 256.4/400.7 MB 5.4 MB/s eta 0:00:27\n",
      "     ------------------------ ------------- 257.7/400.7 MB 5.4 MB/s eta 0:00:27\n",
      "     ------------------------ ------------- 258.7/400.7 MB 5.4 MB/s eta 0:00:27\n",
      "     ------------------------ ------------- 260.3/400.7 MB 5.4 MB/s eta 0:00:26\n",
      "     ------------------------ ------------- 261.6/400.7 MB 5.4 MB/s eta 0:00:26\n",
      "     ------------------------ ------------- 262.9/400.7 MB 5.4 MB/s eta 0:00:26\n",
      "     ------------------------- ------------ 264.0/400.7 MB 5.4 MB/s eta 0:00:26\n",
      "     ------------------------- ------------ 265.0/400.7 MB 5.4 MB/s eta 0:00:25\n",
      "     ------------------------- ------------ 266.6/400.7 MB 5.4 MB/s eta 0:00:25\n",
      "     ------------------------- ------------ 267.6/400.7 MB 5.4 MB/s eta 0:00:25\n",
      "     ------------------------- ------------ 269.0/400.7 MB 5.4 MB/s eta 0:00:25\n",
      "     ------------------------- ------------ 270.3/400.7 MB 5.4 MB/s eta 0:00:24\n",
      "     ------------------------- ------------ 271.6/400.7 MB 5.5 MB/s eta 0:00:24\n",
      "     ------------------------- ------------ 272.6/400.7 MB 5.5 MB/s eta 0:00:24\n",
      "     ------------------------- ------------ 273.9/400.7 MB 5.5 MB/s eta 0:00:24\n",
      "     -------------------------- ----------- 275.3/400.7 MB 5.5 MB/s eta 0:00:23\n",
      "     -------------------------- ----------- 276.0/400.7 MB 5.5 MB/s eta 0:00:23\n",
      "     -------------------------- ----------- 277.6/400.7 MB 5.5 MB/s eta 0:00:23\n",
      "     -------------------------- ----------- 278.9/400.7 MB 5.5 MB/s eta 0:00:23\n",
      "     -------------------------- ----------- 279.7/400.7 MB 5.5 MB/s eta 0:00:22\n",
      "     -------------------------- ----------- 280.8/400.7 MB 5.5 MB/s eta 0:00:22\n",
      "     -------------------------- ----------- 282.1/400.7 MB 5.5 MB/s eta 0:00:22\n",
      "     -------------------------- ----------- 283.4/400.7 MB 5.5 MB/s eta 0:00:22\n",
      "     -------------------------- ----------- 284.4/400.7 MB 5.5 MB/s eta 0:00:22\n",
      "     --------------------------- ---------- 285.5/400.7 MB 5.5 MB/s eta 0:00:21\n",
      "     --------------------------- ---------- 286.8/400.7 MB 5.6 MB/s eta 0:00:21\n",
      "     --------------------------- ---------- 288.4/400.7 MB 5.6 MB/s eta 0:00:21\n",
      "     --------------------------- ---------- 289.1/400.7 MB 5.6 MB/s eta 0:00:21\n",
      "     --------------------------- ---------- 290.7/400.7 MB 5.6 MB/s eta 0:00:20\n",
      "     --------------------------- ---------- 291.8/400.7 MB 5.6 MB/s eta 0:00:20\n",
      "     --------------------------- ---------- 292.8/400.7 MB 5.6 MB/s eta 0:00:20\n",
      "     --------------------------- ---------- 294.4/400.7 MB 5.6 MB/s eta 0:00:20\n",
      "     ---------------------------- --------- 295.4/400.7 MB 5.6 MB/s eta 0:00:19\n",
      "     ---------------------------- --------- 296.5/400.7 MB 5.6 MB/s eta 0:00:19\n",
      "     ---------------------------- --------- 297.5/400.7 MB 5.6 MB/s eta 0:00:19\n",
      "     ---------------------------- --------- 298.3/400.7 MB 5.6 MB/s eta 0:00:19\n",
      "     ---------------------------- --------- 298.6/400.7 MB 5.5 MB/s eta 0:00:19\n",
      "     ---------------------------- --------- 299.4/400.7 MB 5.5 MB/s eta 0:00:19\n",
      "     ---------------------------- --------- 300.7/400.7 MB 5.5 MB/s eta 0:00:19\n",
      "     ---------------------------- --------- 302.0/400.7 MB 5.5 MB/s eta 0:00:18\n",
      "     ---------------------------- --------- 303.6/400.7 MB 5.5 MB/s eta 0:00:18\n",
      "     ---------------------------- --------- 304.3/400.7 MB 5.5 MB/s eta 0:00:18\n",
      "     ---------------------------- --------- 305.7/400.7 MB 5.6 MB/s eta 0:00:18\n",
      "     ----------------------------- -------- 307.0/400.7 MB 5.6 MB/s eta 0:00:17\n",
      "     ----------------------------- -------- 308.0/400.7 MB 5.5 MB/s eta 0:00:17\n",
      "     ----------------------------- -------- 309.3/400.7 MB 5.6 MB/s eta 0:00:17\n",
      "     ----------------------------- -------- 310.9/400.7 MB 5.6 MB/s eta 0:00:17\n",
      "     ----------------------------- -------- 312.2/400.7 MB 5.6 MB/s eta 0:00:16\n",
      "     ----------------------------- -------- 313.0/400.7 MB 5.6 MB/s eta 0:00:16\n",
      "     ----------------------------- -------- 313.3/400.7 MB 5.6 MB/s eta 0:00:16\n",
      "     ----------------------------- -------- 314.3/400.7 MB 5.6 MB/s eta 0:00:16\n",
      "     ----------------------------- -------- 314.8/400.7 MB 5.5 MB/s eta 0:00:16\n",
      "     ----------------------------- -------- 316.1/400.7 MB 5.5 MB/s eta 0:00:16\n",
      "     ------------------------------ ------- 317.5/400.7 MB 5.5 MB/s eta 0:00:15\n",
      "     ------------------------------ ------- 318.8/400.7 MB 5.6 MB/s eta 0:00:15\n",
      "     ------------------------------ ------- 319.8/400.7 MB 5.6 MB/s eta 0:00:15\n",
      "     ------------------------------ ------- 321.1/400.7 MB 5.6 MB/s eta 0:00:15\n",
      "     ------------------------------ ------- 322.4/400.7 MB 5.6 MB/s eta 0:00:15\n",
      "     ------------------------------ ------- 323.7/400.7 MB 5.6 MB/s eta 0:00:14\n",
      "     ------------------------------ ------- 324.8/400.7 MB 5.6 MB/s eta 0:00:14\n",
      "     ------------------------------ ------- 326.4/400.7 MB 5.6 MB/s eta 0:00:14\n",
      "     ------------------------------- ------ 327.7/400.7 MB 5.6 MB/s eta 0:00:14\n",
      "     ------------------------------- ------ 328.5/400.7 MB 5.6 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 329.8/400.7 MB 5.6 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 331.4/400.7 MB 5.6 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 332.4/400.7 MB 5.6 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 333.4/400.7 MB 5.6 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 335.0/400.7 MB 5.6 MB/s eta 0:00:12\n",
      "     ------------------------------- ------ 336.3/400.7 MB 5.6 MB/s eta 0:00:12\n",
      "     ------------------------------- ------ 337.4/400.7 MB 5.6 MB/s eta 0:00:12\n",
      "     -------------------------------- ----- 339.0/400.7 MB 5.6 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 340.3/400.7 MB 5.6 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 341.3/400.7 MB 5.6 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 342.6/400.7 MB 5.6 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 343.9/400.7 MB 5.6 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 345.0/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     -------------------------------- ----- 346.3/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     -------------------------------- ----- 347.9/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 349.2/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 349.2/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 349.2/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 349.4/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 349.4/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 349.4/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 349.4/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 349.4/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 349.4/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 349.4/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 349.4/400.7 MB 5.6 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 351.5/400.7 MB 5.3 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 352.3/400.7 MB 5.3 MB/s eta 0:00:10\n",
      "     --------------------------------- ---- 353.6/400.7 MB 5.3 MB/s eta 0:00:09\n",
      "     --------------------------------- ---- 354.9/400.7 MB 5.3 MB/s eta 0:00:09\n",
      "     --------------------------------- ---- 356.0/400.7 MB 5.3 MB/s eta 0:00:09\n",
      "     --------------------------------- ---- 357.6/400.7 MB 5.3 MB/s eta 0:00:09\n",
      "     ---------------------------------- --- 358.9/400.7 MB 5.3 MB/s eta 0:00:08\n",
      "     ---------------------------------- --- 359.9/400.7 MB 5.3 MB/s eta 0:00:08\n",
      "     ---------------------------------- --- 361.2/400.7 MB 5.3 MB/s eta 0:00:08\n",
      "     ---------------------------------- --- 362.5/400.7 MB 5.3 MB/s eta 0:00:08\n",
      "     ---------------------------------- --- 363.9/400.7 MB 5.3 MB/s eta 0:00:07\n",
      "     ---------------------------------- --- 364.9/400.7 MB 5.3 MB/s eta 0:00:07\n",
      "     ---------------------------------- --- 366.2/400.7 MB 5.4 MB/s eta 0:00:07\n",
      "     ---------------------------------- --- 367.5/400.7 MB 5.4 MB/s eta 0:00:07\n",
      "     ---------------------------------- --- 368.6/400.7 MB 5.4 MB/s eta 0:00:06\n",
      "     ----------------------------------- -- 369.9/400.7 MB 5.4 MB/s eta 0:00:06\n",
      "     ----------------------------------- -- 371.2/400.7 MB 5.4 MB/s eta 0:00:06\n",
      "     ----------------------------------- -- 372.2/400.7 MB 5.4 MB/s eta 0:00:06\n",
      "     ----------------------------------- -- 373.3/400.7 MB 5.4 MB/s eta 0:00:06\n",
      "     ----------------------------------- -- 374.6/400.7 MB 5.4 MB/s eta 0:00:05\n",
      "     ----------------------------------- -- 375.9/400.7 MB 5.4 MB/s eta 0:00:05\n",
      "     ----------------------------------- -- 377.0/400.7 MB 5.4 MB/s eta 0:00:05\n",
      "     ----------------------------------- -- 378.3/400.7 MB 5.4 MB/s eta 0:00:05\n",
      "     ------------------------------------ - 379.6/400.7 MB 5.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ - 381.2/400.7 MB 5.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ - 381.9/400.7 MB 5.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ - 383.3/400.7 MB 5.4 MB/s eta 0:00:04\n",
      "     ------------------------------------ - 384.8/400.7 MB 5.4 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 385.9/400.7 MB 5.4 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 386.9/400.7 MB 5.4 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 388.5/400.7 MB 5.4 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 389.8/400.7 MB 5.4 MB/s eta 0:00:03\n",
      "     -------------------------------------  390.9/400.7 MB 5.4 MB/s eta 0:00:02\n",
      "     -------------------------------------  391.9/400.7 MB 5.4 MB/s eta 0:00:02\n",
      "     -------------------------------------  393.2/400.7 MB 5.4 MB/s eta 0:00:02\n",
      "     -------------------------------------  394.3/400.7 MB 5.4 MB/s eta 0:00:02\n",
      "     -------------------------------------  395.6/400.7 MB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  396.9/400.7 MB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  398.2/400.7 MB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  399.0/400.7 MB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  400.3/400.7 MB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  400.6/400.7 MB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  400.6/400.7 MB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  400.6/400.7 MB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  400.6/400.7 MB 5.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 400.7/400.7 MB 5.2 MB/s  0:01:19\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2d7f0cc-edd0-4d38-8de0-87cb9aeda3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b917b0-4ba5-4510-9605-d496f78686d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Built machine learning pipelines in Python with sp...\" with entities \"[(37, 43, 'SKILL'), (49, 54, 'SKILL'), (59, 66, 'S...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Database work with MySQL, MongoDB, and Redis cachi...\" with entities \"[(20, 25, 'SKILL'), (27, 34, 'SKILL'), (40, 45, 'S...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Worked on Nodejs, Express, REST APIs, MongoDB, and...\" with entities \"[(8, 14, 'SKILL'), (16, 23, 'SKILL'), (25, 33, 'SK...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Front-end work with ReactJS, JSX, and Bootstrap.\" with entities \"[(17, 24, 'SKILL'), (26, 29, 'SKILL'), (35, 44, 'S...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Implemented authentication using JWT and tested AP...\" with entities \"[(35, 38, 'SKILL'), (58, 65, 'SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 ‚Äî Losses: {'ner': 4.419294722876495}\n",
      "Epoch 2/15 ‚Äî Losses: {'ner': 3.4025006853103212}\n",
      "Epoch 3/15 ‚Äî Losses: {'ner': 3.649885867547811}\n",
      "Epoch 4/15 ‚Äî Losses: {'ner': 3.42407873951669}\n",
      "Epoch 5/15 ‚Äî Losses: {'ner': 3.791662938986466}\n",
      "Epoch 6/15 ‚Äî Losses: {'ner': 3.5082755368671643}\n",
      "Epoch 7/15 ‚Äî Losses: {'ner': 3.236031970954714}\n",
      "Epoch 8/15 ‚Äî Losses: {'ner': 2.581859796709697}\n",
      "Epoch 9/15 ‚Äî Losses: {'ner': 2.440496381751922}\n",
      "Epoch 10/15 ‚Äî Losses: {'ner': 2.205741116776898}\n",
      "Epoch 11/15 ‚Äî Losses: {'ner': 2.8437795902272622}\n",
      "Epoch 12/15 ‚Äî Losses: {'ner': 2.590886679332111}\n",
      "Epoch 13/15 ‚Äî Losses: {'ner': 2.107688010760314}\n",
      "Epoch 14/15 ‚Äî Losses: {'ner': 2.341847686619233}\n",
      "Epoch 15/15 ‚Äî Losses: {'ner': 2.0234971129639083}\n",
      "\n",
      "--- Resume 1 ---\n",
      "üßë Personal Info: {'name': 'ALICE JOHNSON ...', 'email': '', 'phone': '', 'linkedin': '', 'github': ''}\n",
      "‚úÖ Known Skills: []\n",
      "‚ùì Unknown Skills: []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "import re\n",
    "\n",
    "# -------------------------------\n",
    "# 1Ô∏è‚É£ Skill variations (expanded)\n",
    "# -------------------------------\n",
    "skill_variations = {\n",
    "    \"node.js\": [\"node.js\", \"nodejs\", \"nodeJs\", \"NODE.JS\"],\n",
    "    \"react.js\": [\"react.js\", \"reactjs\", \"reactJs\", \"React\", \"REACTJS\", \"React native\"],\n",
    "    \"express.js\": [\"express.js\", \"express\", \"Express.js\", \"Express.JS\"],\n",
    "    \"mysql\": [\"mysql\", \"MySQL\"],\n",
    "    \"mongodb\": [\"mongodb\", \"mongoDB\", \"MongoDb\"],\n",
    "    \"vite\": [\"vite\", \"VITE\"],\n",
    "    \"jwt\": [\"jwt\", \"JWT\"],\n",
    "    \"postman\": [\"postman\", \"POSTMAN\"],\n",
    "    \"aws lambda\": [\"aws lambda\", \"AWS Lambda\", \"Lambda\"],\n",
    "    \"aws cognito\": [\"aws cognito\", \"Cognito\"],\n",
    "    \"dynamodb\": [\"dynamodb\", \"DynamoDB\"],\n",
    "    \"python\": [\"python\", \"Python\"],\n",
    "    \"spacy\": [\"spacy\", \"spaCy\"],\n",
    "    \"librosa\": [\"librosa\", \"Librosa\"],\n",
    "    \"bootstrap\": [\"bootstrap\", \"Bootstrap\"],\n",
    "    \"jsx\": [\"jsx\", \"JSX\"],\n",
    "    \"rest api\": [\"rest api\", \"REST API\", \"REST APIs\"],\n",
    "    \"redis\": [\"redis\", \"Redis\"],\n",
    "    \"css3\": [\"css3\", \"CSS3\"]\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 2Ô∏è‚É£ Load transformer-based SpaCy model\n",
    "# -------------------------------\n",
    "nlp = spacy.load(\"en_core_web_lg\")  # stronger transformer model\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "for skill in skill_variations:\n",
    "    if skill.upper() not in ner.labels:\n",
    "        ner.add_label(\"SKILL\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ Expanded TRAIN_DATA\n",
    "# -------------------------------\n",
    "TRAIN_DATA = [\n",
    "    (\"Built APIs with Node.js, Express.js, and MySQL.\", {\"entities\": [(14, 21, \"SKILL\"), (23, 32, \"SKILL\"), (38, 43, \"SKILL\")]}),\n",
    "    (\"Developed cloud functions using AWS Lambda and DynamoDB.\", {\"entities\": [(32, 42, \"SKILL\"), (47, 55, \"SKILL\")]}),\n",
    "    (\"Created web apps with React.js, Vite, and Bootstrap.\", {\"entities\": [(20, 27, \"SKILL\"), (29, 33, \"SKILL\"), (39, 48, \"SKILL\")]}),\n",
    "    (\"Implemented authentication using JWT and tested APIs via Postman.\", {\"entities\": [(35, 38, \"SKILL\"), (58, 65, \"SKILL\")]}),\n",
    "    (\"Front-end work with ReactJS, JSX, and Bootstrap.\", {\"entities\": [(17, 24, \"SKILL\"), (26, 29, \"SKILL\"), (35, 44, \"SKILL\")]}),\n",
    "    (\"Worked on Nodejs, Express, REST APIs, MongoDB, and MySQL.\", {\"entities\": [(8, 14, \"SKILL\"), (16, 23, \"SKILL\"), (25, 33, \"SKILL\"), (35, 42, \"SKILL\"), (48, 53, \"SKILL\")]}),\n",
    "    (\"Built machine learning pipelines in Python with spaCy and Librosa.\", {\"entities\": [(37, 43, \"SKILL\"), (49, 54, \"SKILL\"), (59, 66, \"SKILL\")]}),\n",
    "    (\"Database work with MySQL, MongoDB, and Redis caching.\", {\"entities\": [(20, 25, \"SKILL\"), (27, 34, \"SKILL\"), (40, 45, \"SKILL\")]}),\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ Fine-tune NER\n",
    "# -------------------------------\n",
    "if TRAIN_DATA:\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.resume_training()\n",
    "        n_epochs = 15  # more epochs for better learning\n",
    "        for epoch in range(n_epochs):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(2.0, 16.0, 1.5))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                examples = [Example.from_dict(nlp.make_doc(t), a) for t, a in batch]\n",
    "                nlp.update(examples, sgd=optimizer, drop=0.2, losses=losses)\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} ‚Äî Losses: {losses}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ PhraseMatcher for skill variations\n",
    "# -------------------------------\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "for skill, variations in skill_variations.items():\n",
    "    patterns = [nlp.make_doc(v) for v in variations]\n",
    "    matcher.add(skill, patterns)\n",
    "\n",
    "# -------------------------------\n",
    "# 6Ô∏è‚É£ Text cleaning\n",
    "# -------------------------------\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"[\\(\\)|,]\", \" \", text)  # remove (, | ,)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.lower()\n",
    "\n",
    "# -------------------------------\n",
    "# 7Ô∏è‚É£ Skill extraction\n",
    "# -------------------------------\n",
    "def extract_skills(text):\n",
    "    text_clean = clean_text(text)\n",
    "    doc = nlp(text_clean)\n",
    "\n",
    "    matched_skills = set()\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        skill_label = nlp.vocab.strings[match_id]\n",
    "        matched_skills.add(skill_label.lower())\n",
    "\n",
    "    ner_skills = set([ent.text.lower() for ent in doc.ents if ent.label_ == \"SKILL\"])\n",
    "    all_skills = matched_skills.union(ner_skills)\n",
    "\n",
    "    known = sorted([s for s in all_skills if s in skill_variations])\n",
    "    unknown = sorted([s for s in all_skills if s not in skill_variations])\n",
    "    return known, unknown\n",
    "\n",
    "# -------------------------------\n",
    "# 8Ô∏è‚É£ Personal info extraction\n",
    "# -------------------------------\n",
    "def extract_personal_info(text):\n",
    "    lines = text.splitlines()\n",
    "    name = \"\"\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and re.match(r\"^[A-Za-z\\s\\-\\.]+$\", line):\n",
    "            name = line\n",
    "            break\n",
    "    email = re.search(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "    phone = re.search(r\"(\\+?\\d[\\d\\s-]{7,}\\d)\", text)\n",
    "    linkedin = re.search(r\"https?://(www\\.)?linkedin\\.com/[^\\s,]+\", text)\n",
    "    github = re.search(r\"https?://(www\\.)?github\\.com/[^\\s,]+\", text)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"email\": email.group(0) if email else \"\",\n",
    "        \"phone\": phone.group(0) if phone else \"\",\n",
    "        \"linkedin\": linkedin.group(0) if linkedin else \"\",\n",
    "        \"github\": github.group(0) if github else \"\"\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 9Ô∏è‚É£ Test resumes\n",
    "# -------------------------------\n",
    "test_resumes = [\n",
    "    \"\"\"ALICE JOHNSON ...\"\"\",  # same resumes as before\n",
    "]\n",
    "\n",
    "for i, resume in enumerate(test_resumes, 1):\n",
    "    print(f\"\\n--- Resume {i} ---\")\n",
    "    personal_info = extract_personal_info(resume)\n",
    "    known_skills_found, unknown_skills_found = extract_skills(resume)\n",
    "    print(\"üßë Personal Info:\", personal_info)\n",
    "    print(\"‚úÖ Known Skills:\", known_skills_found)\n",
    "    print(\"‚ùì Unknown Skills:\", unknown_skills_found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ce7f6-d031-401d-86dc-69f9a17b3fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
